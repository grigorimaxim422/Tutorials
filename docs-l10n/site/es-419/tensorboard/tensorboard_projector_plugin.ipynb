{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFloNx163DCr"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iSdwTGPc3Hpj"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE2AKncl3QJZ"
      },
      "source": [
        "# Visualización de datos mediante el Proyector de incorporaciones de TensorBoard\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tensorboard/tensorboard_projector_plugin.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tensorboard/tensorboard_projector_plugin.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tensorboard/tensorboard_projector_plugin.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4s3Sf2I3mJr"
      },
      "source": [
        "## Descripción general\n",
        "\n",
        "Utilizando el **Proyector de incorporaciones de TensorBoard**, puede representar gráficamente incorporaciones de alta dimensión. Esto puede ser útil para visualizar, examinar y comprender sus capas de incorporación.\n",
        "\n",
        "En este tutorial, aprenderá a visualizar este tipo de capa entrenada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-0rhuaW9f2-"
      },
      "source": [
        "## Preparación\n",
        "\n",
        "En este tutorial, utilizaremos TensorBoard para visualizar una capa de incorporación generada para clasificar datos de reseñas de películas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjRkD3r3etuL"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh22cCoM8t7e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorboard.plugins import projector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlp6ZASQB5go"
      },
      "source": [
        "## Datos de IMDB\n",
        "\n",
        "Utilizaremos un conjunto de datos de 25,000 críticas de películas de IMDB, cada una de las cuales tiene una etiqueta de sentimiento (positivo/negativo). Cada crítica se preprocesa y se codifica como una secuencia de índices de palabras (números enteros). Para simplificar, las palabras se indizan por frecuencia global en el conjunto de datos; por ejemplo, el entero \"3\" codifica la 3ª palabra más frecuente que aparece en todas las reseñas. Esto permite realizar rápidamente operaciones de filtrado como: \"considerar sólo las 10,000 palabras más frecuentes, pero eliminando las 20 palabras más frecuentes\".\n",
        "\n",
        "Como convención, \"0\" no representa ninguna palabra específica, sino que se utiliza para codificar cualquier palabra desconocida. Más adelante en el tutorial, eliminaremos la fila para el \"0\" en la visualización.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0Yiw05gIgqS"
      },
      "outputs": [],
      "source": [
        "(train_data, test_data), info = tfds.load(\n",
        "    \"imdb_reviews/subwords8k\",\n",
        "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "encoder = info.features[\"text\"].encoder\n",
        "\n",
        "# Shuffle and pad the data.\n",
        "train_batches = train_data.shuffle(1000).padded_batch(\n",
        "    10, padded_shapes=((None,), ())\n",
        ")\n",
        "test_batches = test_data.shuffle(1000).padded_batch(\n",
        "    10, padded_shapes=((None,), ())\n",
        ")\n",
        "train_batch, train_labels = next(iter(train_batches))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpvPVCwO7bDj"
      },
      "source": [
        "# Capa de incorporación de Keras\n",
        "\n",
        "Una [Capa de incorporación de Keras](https://keras.io/layers/embeddings/) puede utilizarse para entrenar una incorporación para cada palabra de su vocabulario. Cada palabra (o subpalabra en este caso) se asociará a un vector de 16 dimensiones (o incorporación) que será entrenado por el modelo.\n",
        "\n",
        "Consulte [este tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings?hl=en) para obtener más información sobre la incorporación de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fgoq5haqw8Z5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2500/2500 [==============================] - 13s 5ms/step - loss: 0.5330 - accuracy: 0.6769 - val_loss: 0.4043 - val_accuracy: 0.7800\n"
          ]
        }
      ],
      "source": [
        "# Create an embedding layer.\n",
        "embedding_dim = 16\n",
        "embedding = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim)\n",
        "# Configure the embedding layer as part of a keras model.\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        embedding, # The embedding layer should be the first layer in a model.\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile model.\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Train model for one epoch.\n",
        "history = model.fit(\n",
        "    train_batches, epochs=1, validation_data=test_batches, validation_steps=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9HmC29hdMnH"
      },
      "source": [
        "## Guardando datos para TensorBoard\n",
        "\n",
        "TensorBoard lee tensores y metadatos de los registros de sus proyectos de tensorflow. La ruta al directorio de registros se especifica a continuación con `log_dir`. Para este tutorial, utilizaremos `/logs/imdb-example/`.\n",
        "\n",
        "Para cargar los datos en Tensorboard, necesitamos guardar un punto de verificación del entrenamiento en ese directorio, junto con metadatos que permitan visualizar una capa específica de interés en el modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi8_SCYRdn9x"
      },
      "outputs": [],
      "source": [
        "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
        "log_dir='/logs/imdb-example/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Save Labels separately on a line-by-line manner.\n",
        "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
        "  for subwords in encoder.subwords:\n",
        "    f.write(\"{}\\n\".format(subwords))\n",
        "  # Fill in the rest of the labels with \"unknown\".\n",
        "  for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
        "    f.write(\"unknown #{}\\n\".format(unknown))\n",
        "\n",
        "\n",
        "# Save the weights we want to analyze as a variable. Note that the first\n",
        "# value represents any unknown word, which is not in the metadata, here\n",
        "# we will remove this value.\n",
        "weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n",
        "# Create a checkpoint from embedding, the filename and key are the\n",
        "# name of the tensor.\n",
        "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
        "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
        "\n",
        "# Set up config.\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "embedding.metadata_path = 'metadata.tsv'\n",
        "projector.visualize_embeddings(log_dir, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtL_KzYMBIzP"
      },
      "outputs": [],
      "source": [
        "# Now run tensorboard against on log data we just saved.\n",
        "%tensorboard --logdir /logs/imdb-example/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtzW8mr_wmbD"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/embedding_projector.png?raw=1\"/> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG4hcUzQQoWA"
      },
      "source": [
        "## Análisis\n",
        "\n",
        "El proyector TensorBoard es una gran herramienta para interpretar y visualizar la incorporación. El panel de control permite a los usuarios buscar términos específicos y resalta las palabras que son adyacentes entre sí en el espacio de incorporación (de baja dimensión). En este ejemplo podemos ver que Wes **Anderson** y Alfred **Hitchcock** son ambos términos bastante neutros, pero que se hace referencia a ellos en contextos diferentes.\n",
        "\n",
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/embedding_projector_hitchcock.png?raw=1\"/> -->\n",
        "\n",
        "En este espacio, Hitchcock está más cerca de palabras como `nightmare`, lo que probablemente se deba al hecho de que se le conoce como el \"Maestro del suspenso\", mientras que Anderson está más cerca de la palabra `heart`, que es coherente con su estilo implacablemente detallista y conmovedor.\n",
        "\n",
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/embedding_projector_anderson.png?raw=1\"/> -->"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tensorboard_projector_plugin.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
