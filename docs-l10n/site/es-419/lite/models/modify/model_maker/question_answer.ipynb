{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Respuesta a preguntas BERT vía Model Maker de TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw5Y7snSuG51"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/question_answer\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/lite/models/modify/model_maker/question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/lite/models/modify/model_maker/question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/lite/models/modify/model_maker/question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr3q-gvm3cI8"
      },
      "source": [
        "La librería [Model Maker de TensorFlow Lite](https://www.tensorflow.org/lite/models/modify/model_maker) simplifica el proceso de adaptación y conversión de un modelo TensorFlow a unos datos de entrada concretos cuando se implementa este modelo para aplicaciones de ML en dispositivos.\n",
        "\n",
        "Este bloc muestra un ejemplo de principio a fin que utiliza la librería Model Maker para ilustrar la adaptación y conversión de un modelo de pregunta-respuesta de uso común para una tarea de pregunta-respuesta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxEHFTk755qw"
      },
      "source": [
        "# Introducción a la tarea de Respuesta a preguntas BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFbKTCF25-SG"
      },
      "source": [
        "La tarea admitida en esta librería es la de pregunta-respuesta extractiva, lo que significa que dado un pasaje y una pregunta, la respuesta es el alcance en el pasaje. La imagen siguiente muestra un ejemplo de pregunta-respuesta.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_squad_showcase.png\" width=\"500\"></p>\n",
        "\n",
        "<p align=\"center\">\n",
        "    <em>Las respuestas son bloques en el pasaje (crédito de la imagen: <a href=\"https://rajpurkar.github.io/mlx/qa-and-squad/\">SQuAD blog</a>)</em>\n",
        "</p>\n",
        "\n",
        "En cuanto al modelo de tarea de pregunta-respuesta, las entradas deben ser el pasaje y el par de preguntas ya preprocesados, las salidas deben ser los logits de inicio y los logits finales de cada token del pasaje. El tamaño de la entrada podría fijarse y ajustarse en función de la longitud del pasaje y de la pregunta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb7P4WQta8Ub"
      },
      "source": [
        "## Visión general de principio a fin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7cIHjIfbDlG"
      },
      "source": [
        "El siguiente fragmento de código demuestra cómo entrar en el modelo con unas pocas líneas de código. El proceso general incluye 5 pasos: (1) seleccionar un modelo, (2) cargar datos, (3) reentrenar el modelo, (4) evaluar, y (5) exportarlo al formato TensorFlow Lite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQPdlxZBYuZG"
      },
      "source": [
        "```python\n",
        "# Chooses a model specification that represents the model.\n",
        "spec = model_spec.get('mobilebert_qa')\n",
        "\n",
        "# Gets the training data and validation data.\n",
        "train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)\n",
        "validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)\n",
        "\n",
        "# Fine-tunes the model.\n",
        "model = question_answer.create(train_data, model_spec=spec)\n",
        "\n",
        "# Gets the evaluation result.\n",
        "metric = model.evaluate(validation_data)\n",
        "\n",
        "# Exports the model to the TensorFlow Lite format with metadata in the export directory.\n",
        "model.export(export_dir)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exScAdvBbNEi"
      },
      "source": [
        "Las siguientes secciones explican el código con más detalle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## Requisitos previos\n",
        "\n",
        "Para ejecutar este ejemplo, instale los paquetes necesarios, incluido el paquete Model Maker del repositorio [GitHub](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhl8lqVamEty"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install -q tflite-model-maker-nightly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6lRhVK9Q_0U"
      },
      "source": [
        "Importe los paquetes necesarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import question_answer\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker.question_answer import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l65ctmtW7_FF"
      },
      "source": [
        "La \"Visión general de principio a fin\" muestra un sencillo ejemplo de este tipo. Las secciones siguientes recorren el ejemplo paso a paso para mostrar más detalles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ_B8fMDOhMR"
      },
      "source": [
        "## Elegir un model_spec que represente un modelo para la respuesta a la pregunta\n",
        "\n",
        "Cada objeto `model_spec` representa un modelo específico para responder a una pregunta. El Model Maker admite actualmente los modelos MobileBERT y BERT-Base.\n",
        "\n",
        "Modelo compatible | Nombre de model_spec | Descripción del modelo\n",
        "--- | --- | ---\n",
        "[MobileBERT](https://arxiv.org/pdf/2004.02984.pdf) | 'mobilebert_qa' | 4.3 veces más pequeño y 5.5 veces más rápido que la BERT-Base logrando resultados competitivos, adecuado para el escenario en el dispositivo.\n",
        "[MobileBERT-SQuAD](https://arxiv.org/pdf/2004.02984.pdf) | 'mobilebert_qa_squad' | La misma arquitectura de modelo que el modelo MobileBERT y el modelo inicial ya está reentrenado en [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/).\n",
        "[BERT-Base](https://arxiv.org/pdf/1810.04805.pdf) | 'bert_qa' | Modelo BERT estándar muy utilizado en tareas de PNL.\n",
        "\n",
        "En este tutorial, se utiliza [MobileBERT-SQuAD](https://arxiv.org/pdf/2004.02984.pdf) como ejemplo. Dado que el modelo ya está reentrenado en [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/), podría tener una cobertura más rápida para la tarea de respuesta a preguntas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEAWuZQ1PFiX"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('mobilebert_qa_squad')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygEncJxtl-nQ"
      },
      "source": [
        "## Cargar datos de entrada específicos para una aplicación de ML en el dispositivo y preprocesar los datos\n",
        "\n",
        "El [TriviaQA](https://nlp.cs.washington.edu/triviaqa/) es un conjunto de datos de Comprensión lectora que contiene más de 650K tripletas pregunta-respuesta-evidencia. En este tutorial, usará un subconjunto de este conjunto de datos para aprender a usar la librería Model Maker.\n",
        "\n",
        "Para cargar los datos, convierta el conjunto de datos TriviaQA al formato [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/) ejecutando el [script Python del convertidor](https://github.com/mandarjoshi90/triviaqa#miscellaneous) con `--sample_size=8000` y un conjunto de datos `web`. Modifique un poco el código de conversión:\n",
        "\n",
        "- Omitiendo los muestreos en los que no pudo encontrar ninguna respuesta en el documento de contexto;\n",
        "- Entrar en la respuesta original en el contexto sin mayúsculas ni minúsculas.\n",
        "\n",
        "Descargue la versión archivada del conjunto de datos ya convertido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tOfUr2KlgpU"
      },
      "outputs": [],
      "source": [
        "train_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-web-train-8000.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-web-train-8000.json')\n",
        "validation_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-verified-web-dev.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-verified-web-dev.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfZk8GNr_1nc"
      },
      "source": [
        "También puede entrenar el modelo MobileBERT con su propio conjunto de datos. Si está ejecutando este bloc en Colab, cargue sus datos usando la barra lateral izquierda.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_question_answer.png\" width=\"800\" hspace=\"100\" alt=\"Subir archivo\">\n",
        "\n",
        "Si prefiere no subir sus datos a la nube, también puede ejecutar la librería sin conexión siguiendo la [guía](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E051HBUM5owi"
      },
      "source": [
        "Use el método `DataLoader.from_squad` para cargar y preprocesar los datos del formato [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) de acuerdo con un `model_spec` específico. Puede usar los formatos SQuAD2.0 o SQuAD1.1. Establecer el parámetro `version_2_with_negative` como `True` significa que el formato es SQuAD2.0. En caso contrario, el formato es SQuAD1.1. De forma predeterminada, `version_2_with_negative` es `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_fOlZsklmlL"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)\n",
        "validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWuoensX4vDA"
      },
      "source": [
        "## Personalice el modelo TensorFlow.\n",
        "\n",
        "Cree un modelo personalizado de pregunta-respuesta basado en los datos cargados. La función `create` comprende los siguientes pasos:\n",
        "\n",
        "1. Crea el modelo de respuesta a la pregunta según `model_spec`.\n",
        "2. Entrene el modelo de pregunta-respuesta. Las épocas predeterminadas y el tamaño predeterminado del lote se establecen según dos variables `default_training_epochs` y `default_batch_size` en el objeto `model_spec`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvYSUuJY3QxR"
      },
      "outputs": [],
      "source": [
        "model = question_answer.create(train_data, model_spec=spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JKI-pNc8idH"
      },
      "source": [
        "Vea la estructura detallada del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd7Hs8TF8n3H"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP5FPk_tOxoZ"
      },
      "source": [
        "## Evalúe el modelo personalizado\n",
        "\n",
        "Evalúe el modelo en los datos de validación y obtenga un dictado de métricas que incluyan `f1` puntuaciones y `exact match` etc. Tenga en cuenta que las métricas son diferentes para SQuAD1.1 y SQuAD2.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8c2ZQ0J3Riy"
      },
      "outputs": [],
      "source": [
        "model.evaluate(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHoGAceO2xV"
      },
      "source": [
        "## Exporte al modelo TensorFlow Lite.\n",
        "\n",
        "Convierta el modelo entrenado al formato de modelo TensorFlow Lite con [metadatos](https://www.tensorflow.org/lite/models/convert/metadata) para poder usarlo posteriormente en una aplicación de ML en el dispositivo. El archivo de vocabulario está incrustado en los metadatos. El nombre de archivo TFLite predeterminado es `model.tflite`.\n",
        "\n",
        "En muchas aplicaciones de ML en el dispositivo, el tamaño del modelo es un factor importante. Por lo tanto, se recomienda aplicar la cuantización del modelo para hacerlo más pequeño y, potencialmente, ejecutarlo más rápido. La técnica de cuantización predeterminada tras el entrenamiento es la cuantización de rango dinámico para los modelos BERT y MobileBERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im6wA9lK3TQB"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w12kvDdHJIGH"
      },
      "source": [
        "Puede usar el archivo del modelo de TensorFlow Lite en la app de referencia [bert_qa](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android) usando la [API BertQuestionAnswerer](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_question_answerer) en la [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview) descargándola de la barra lateral izquierda de Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFnJPvq3VGh3"
      },
      "source": [
        "Los formatos de exportación permitidos pueden ser uno o varios de los siguientes:\n",
        "\n",
        "- `ExportFormat.TFLITE`\n",
        "- `ExportFormat.VOCAB`\n",
        "- `ExportFormat.SAVED_MODEL`\n",
        "\n",
        "De forma predeterminada, sólo exporta el modelo TensorFlow Lite con metadatos. También puede exportar selectivamente diferentes archivos. Por ejemplo, se puede exportar sólo el archivo de vocabulario de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro2hz4kXVImY"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', export_format=ExportFormat.VOCAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZKYthlVrTos"
      },
      "source": [
        "También puede evaluar el modelo tflite con el método `evaluate_tflite`. Se espera que este paso le lleve mucho tiempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ochbq95ZrVFX"
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('model.tflite', validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoWiA_zX8rxE"
      },
      "source": [
        "## Uso avanzado\n",
        "\n",
        "La función `create` es la parte crítica de esta librería en la que el parámetro `model_spec` define la especificación del modelo. Actualmente se admite la clase `BertQASpec`. Existen 2 modelos: Modelo MobileBERT, Modelo BERT-Base. La función `create` comprende los siguientes pasos:\n",
        "\n",
        "1. Crea el modelo de respuesta a la pregunta según `model_spec`.\n",
        "2. Entrene el modelo de pregunta-respuesta.\n",
        "\n",
        "Esta sección describe varios temas avanzados, como el ajuste del modelo, el ajuste de los hiperparámetros de entrenamiento, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwtiksguDfhl"
      },
      "source": [
        "### Ajuste el modelo\n",
        "\n",
        "Puede ajustar la infraestructura del modelo como los parámetros `seq_len` y `query_len` en la clase `BertQASpec`.\n",
        "\n",
        "Parámetros ajustables para el modelo:\n",
        "\n",
        "- `seq_len`: Longitud del pasaje a introducir en el modelo.\n",
        "- `query_len`: Longitud de la pregunta para introducirla en el modelo.\n",
        "- `doc_stride`: El intervalo cuando se hace un enfoque de ventana deslizante para tomar trozos de los documentos.\n",
        "- `initializer_range`: El stdev del truncated_normal_initializer para inicializar todas las matrices de ponderación.\n",
        "- `trainable`: Booleano, si la capa preentrenada es entrenable.\n",
        "\n",
        "Parámetros ajustables para la tubería de entrenamiento:\n",
        "\n",
        "- `model_dir`: La ubicación de los archivos de puntos de verificación del modelo. Si no está activa, se usará el directorio temporal.\n",
        "- `dropout_rate`: La tasa de abandono.\n",
        "- `learning_rate`: La tasa de aprendizaje inicial de Adam.\n",
        "- `predict_batch_size`: Tamaño del lote para la predicción.\n",
        "- `tpu`: Dirección TPU a la que conectarse. Sólo se utiliza si se usa la TPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAOd5_bzH9AQ"
      },
      "source": [
        "Por ejemplo, puede entrenar el modelo con una longitud de secuencia mayor. Si cambia el modelo, primero debe construir un nuevo `model_spec`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9WBN0UTQoMN"
      },
      "outputs": [],
      "source": [
        "new_spec = model_spec.get('mobilebert_qa')\n",
        "new_spec.seq_len = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LSTdghTP0Cv"
      },
      "source": [
        "Los pasos restantes son los mismos. Tenga en cuenta que debe volver a ejecutar las partes `dataloader` y `create`, ya que las diferentes especificaciones del modelo pueden tener diferentes pasos de preprocesamiento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQuy7RSDir3"
      },
      "source": [
        "### Ajuste los hiperparámetros de entrenamiento\n",
        "\n",
        "También puede ajustar los hiperparámetros de entrenamiento como `epochs` y `batch_size` para influir en el rendimiento del modelo. Por ejemplo:\n",
        "\n",
        "- `epochs`: Un mayor número de épocas podría lograr un mejor rendimiento, pero podría conducir a un sobreajuste.\n",
        "- `batch_size`: número de muestras a usar en un paso de entrenamiento.\n",
        "\n",
        "Por ejemplo, puede entrenar con más épocas y con un tamaño de lote mayor como:\n",
        "\n",
        "```python\n",
        "model = question_answer.create(train_data, model_spec=spec, epochs=5, batch_size=64)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq6B9lKMfhS6"
      },
      "source": [
        "### Cambiar la arquitectura del modelo\n",
        "\n",
        "Puede cambiar el modelo base sobre el que se entrenan sus datos modificando el `model_spec`. Por ejemplo, para cambiar al modelo BERT-Base, ejecute:\n",
        "\n",
        "```python\n",
        "spec = model_spec.get('bert_qa')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2d7yycrgu6L"
      },
      "source": [
        "Los pasos restantes son los mismos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFQrDMXzOVoB"
      },
      "source": [
        "### Personalice la cuantización posterior al entrenamiento en el modelo TensorFlow Lite\n",
        "\n",
        "[La cuantización posterior al entrenamiento](https://www.tensorflow.org/lite/performance/post_training_quantization) es una técnica de conversión que puede reducir el tamaño del modelo y la latencia de la inferencia, al tiempo que mejora la velocidad de inferencia de la CPU y del acelerador de hardware, con una pequeña degradación de la precisión del modelo. Por ello, se usa mucho para optimizar el modelo.\n",
        "\n",
        "La librería Model Maker aplica una técnica predeterminada de cuantización postentrenamiento al exportar el modelo. Si desea personalizar la cuantización postentrenamiento, Model Maker también soporta múltiples opciones de cuantización postentrenamiento utilizando [QuantizationConfig](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/config/QuantizationConfig). Tomemos como ejemplo la cuantización float16. En primer lugar, defina la configuración de cuantificación.\n",
        "\n",
        "```python\n",
        "config = QuantizationConfig.for_float16()\n",
        "```\n",
        "\n",
        "A continuación, exportamos el modelo TensorFlow Lite con dicha configuración.\n",
        "\n",
        "```python\n",
        "model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPVopCeB6LV6"
      },
      "source": [
        "# Más información\n",
        "\n",
        "Puede leer nuestro ejemplo de [Preguntas y respuestas de BERT](https://www.tensorflow.org/lite/examples/bert_qa/overview) para saber los detalles técnicos. Para más información, consulte:\n",
        "\n",
        "- [Guía](https://www.tensorflow.org/lite/models/modify/model_maker) y [Referencia de API](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker) de Model Maker de TensorFlow Lite.\n",
        "- Librería de tareas: [BertQuestionAnswerer](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_question_answerer) para su implementación.\n",
        "- Las apps de referencia de principio a fin: [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android) y [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/ios)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "question_answer.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
