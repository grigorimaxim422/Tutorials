{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Clasificación de imágenes con Model Maker de TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDABAblytltI"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/image_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/lite/models/modify/model_maker/image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/lite/models/modify/model_maker/image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/lite/models/modify/model_maker/image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelo de TF Hub</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m86-Nh4pMHqY"
      },
      "source": [
        "La librería [Model Maker de TensorFlow Lite](https://www.tensorflow.org/lite/models/modify/model_maker) simplifica el proceso de adaptación y conversión de un modelo de red neuronal TensorFlow a unos datos de entrada concretos cuando se implementa este modelo para aplicaciones de ML en dispositivos.\n",
        "\n",
        "Este bloc muestra un ejemplo de principio a fin que usa esta librería Model Maker para ilustrar la adaptación y conversión de un modelo de clasificación de imágenes de uso común para clasificar flores en un dispositivo móvil."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## Requisitos previos\n",
        "\n",
        "Para ejecutar este ejemplo, primero necesitamos instalar varios paquetes necesarios, incluyendo el paquete Model Maker que se encuentra en [repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) de GitHub ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cv3K3oaksJv"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install -q tflite-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx1HGRoFQ54j"
      },
      "source": [
        "Importe los paquetes necesarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import image_classifier\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker.config import QuantizationConfig\n",
        "from tflite_model_maker.image_classifier import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKRaYHABpob5"
      },
      "source": [
        "## Ejemplo sencillo de principio a fin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiZZ5DHXotaW"
      },
      "source": [
        "### Obtener la ruta de datos\n",
        "\n",
        "Tomemos algunas imágenes para jugar con este sencillo ejemplo de principio a fin. Con cientos de imágenes se puede empezar bien con el Model Maker, mientras que con más datos se podría conseguir una mayor precisión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3jz5x0JoskPv"
      },
      "outputs": [],
      "source": [
        "image_path = tf.keras.utils.get_file(\n",
        "      'flower_photos.tgz',\n",
        "      'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "      extract=True)\n",
        "image_path = os.path.join(os.path.dirname(image_path), 'flower_photos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a55MR6i6nuDm"
      },
      "source": [
        "Podría sustituir `image_path` por sus propias carpetas de imágenes. Respecto a subir datos a colab, puede encontrar el botón de subir en la barra lateral izquierda mostrada en la imagen de abajo con el rectángulo rojo. Pruebe a subir un archivo zip y descomprímalo. La ruta raíz del archivo es la ruta actual.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_image_classification.png\" width=\"800\" hspace=\"100\" alt=\"Subir archivo\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNRNv_mloS89"
      },
      "source": [
        "Si prefiere no subir sus imágenes a la nube, puede intentar ejecutar la librería localmente siguiendo la [guía](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) en GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-VDriAdsowu"
      },
      "source": [
        "### Ejecutar el ejemplo\n",
        "\n",
        "El ejemplo sólo consta de 4 líneas de código, como se muestra a continuación, cada una de las cuales representa un paso del proceso global.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ahtcO86tZBL"
      },
      "source": [
        "Paso 1. Cargue los datos de entrada específicos de una app de ML en el dispositivo. Divídalos en datos de entrenamiento y datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lANoNS_gtdH1"
      },
      "outputs": [],
      "source": [
        "data = DataLoader.from_folder(image_path)\n",
        "train_data, test_data = data.split(0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_9IWyIztuRF"
      },
      "source": [
        "Paso 2. Personalice el modelo TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRXMZbrwtyRD"
      },
      "outputs": [],
      "source": [
        "model = image_classifier.create(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxU2fDr-t2Ya"
      },
      "source": [
        "Paso 3. Evalúe el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQr02VxJt6Cs"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVZw9zU8t84y"
      },
      "source": [
        "Paso 4.  Exporte al modelo TensorFlow Lite.\n",
        "\n",
        "Aquí, exportamos el modelo TensorFlow Lite con [metadatos](https://www.tensorflow.org/lite/models/convert/metadata) que ofrece un estándar para las descripciones del modelo. El archivo de etiquetas está incrustado en los metadatos. La técnica de cuantización predeterminada tras el entrenamiento es la cuantización entera completa para la tarea de clasificación de imágenes.\n",
        "\n",
        "Puede descargarla en la barra lateral izquierda al igual que la parte de carga para su propio uso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb-eIzfluCoa"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyju1qc_v-wy"
      },
      "source": [
        "Después de estos sencillos 4 pasos, podríamos seguir usando el archivo de modelo de TensorFlow Lite en aplicaciones en el dispositivo como en la app de referencia [clasificacion de imágenes](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1QG32ivs9lF"
      },
      "source": [
        "## Proceso detallado\n",
        "\n",
        "Actualmente, admitimos varios modelos como EfficientNet-Lite*, MobileNetV2, ResNet50 a modo de modelos preentrenados para la clasificación de imágenes. Pero resulta muy flexible añadir nuevos modelos preentrenados a esta librería con sólo unas pocas líneas de código.\n",
        "\n",
        "En esta sección se recorre paso a paso este ejemplo de extremo a extremo para ver más detalles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygEncJxtl-nQ"
      },
      "source": [
        "### Paso 1: Cargue los datos de entrada específicos de una app de ML en el dispositivo\n",
        "\n",
        "El conjunto de datos de flores contiene 3670 imágenes correspondientes a 5 clases. Descargue la versión de archivo del conjunto de datos y descomprímala.\n",
        "\n",
        "El conjunto de datos tiene la siguiente estructura de directorios:\n",
        "\n",
        "<pre>\n",
        "&lt;b&gt;flower_photos&lt;/b&gt;\n",
        "|__ &lt;b&gt;daisy&lt;/b&gt;\n",
        "    |______ 100080576_f52e8ee070_n.jpg\n",
        "    |______ 14167534527_781ceb1b7a_n.jpg\n",
        "    |______ ...\n",
        "|__ &lt;b&gt;dandelion&lt;/b&gt;\n",
        "    |______ 10043234166_e6dd915111_n.jpg\n",
        "    |______ 1426682852_e62169221f_m.jpg\n",
        "    |______ ...\n",
        "|__ &lt;b&gt;roses&lt;/b&gt;\n",
        "    |______ 102501987_3cdb8e5394_n.jpg\n",
        "    |______ 14982802401_a3dfb22afb.jpg\n",
        "    |______ ...\n",
        "|__ &lt;b&gt;sunflowers&lt;/b&gt;\n",
        "    |______ 12471791574_bb1be83df4.jpg\n",
        "    |______ 15122112402_cafa41934f.jpg\n",
        "    |______ ...\n",
        "|__ &lt;b&gt;tulips&lt;/b&gt;\n",
        "    |______ 13976522214_ccec508fe7.jpg\n",
        "    |______ 14487943607_651e8062a1_m.jpg\n",
        "    |______ ...\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tOfUr2KlgpU"
      },
      "outputs": [],
      "source": [
        "image_path = tf.keras.utils.get_file(\n",
        "      'flower_photos.tgz',\n",
        "      'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "      extract=True)\n",
        "image_path = os.path.join(os.path.dirname(image_path), 'flower_photos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E051HBUM5owi"
      },
      "source": [
        "Usar la clase `DataLoader` para cargar los datos.\n",
        "\n",
        "En cuanto al método `from_folder()`, podría cargar los datos desde la carpeta. Se supone que los datos de imagen de la misma clase están en el mismo subdirectorio y el nombre de la subcarpeta es el nombre de la clase. Actualmente, se admiten imágenes codificadas en JPEG y en PNG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_fOlZsklmlL"
      },
      "outputs": [],
      "source": [
        "data = DataLoader.from_folder(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u501eT4koURB"
      },
      "source": [
        "Divídalo en datos de entrenamiento (80%), datos de validación (10%, opcional) y datos de prueba (10%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY4UU5SUobtJ"
      },
      "outputs": [],
      "source": [
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9_MYPie3EMO"
      },
      "source": [
        "Muestre 25 ejemplos de imágenes con etiquetas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih4Wx44I482b"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i, (image, label) in enumerate(data.gen_dataset().unbatch().take(25)):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image.numpy(), cmap=plt.cm.gray)\n",
        "  plt.xlabel(data.index_to_label[label.numpy()])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWuoensX4vDA"
      },
      "source": [
        "### Paso 2: Personalice el modelo TensorFlow\n",
        "\n",
        "Cree un modelo clasificador de imágenes personalizado basado en los datos cargados. El modelo predeterminado es EfficientNet-Lite0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvYSUuJY3QxR"
      },
      "outputs": [],
      "source": [
        "model = image_classifier.create(train_data, validation_data=validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JFOKWnH9x8_"
      },
      "source": [
        "Vea la estructura detallada del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNXAfjl192dC"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP5FPk_tOxoZ"
      },
      "source": [
        "### Paso 3: Evalúe el modelo personalizado\n",
        "\n",
        "Evalúe el resultado del modelo, obtenga la pérdida y la precisión del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8c2ZQ0J3Riy"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZCrYOWoCt05"
      },
      "source": [
        "Podemos representar los resultados predichos en 100 imágenes de prueba. Las etiquetas con color rojo son los resultados predichos erróneos, mientras que los demás son correctos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9O9Kx7nDQWD"
      },
      "outputs": [],
      "source": [
        "# A helper function that returns 'red'/'black' depending on if its two input\n",
        "# parameter matches or not.\n",
        "def get_label_color(val1, val2):\n",
        "  if val1 == val2:\n",
        "    return 'black'\n",
        "  else:\n",
        "    return 'red'\n",
        "\n",
        "# Then plot 100 test images and their predicted labels.\n",
        "# If a prediction result is different from the label provided label in \"test\"\n",
        "# dataset, we will highlight it in red color.\n",
        "plt.figure(figsize=(20, 20))\n",
        "predicts = model.predict_top_k(test_data)\n",
        "for i, (image, label) in enumerate(test_data.gen_dataset().unbatch().take(100)):\n",
        "  ax = plt.subplot(10, 10, i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image.numpy(), cmap=plt.cm.gray)\n",
        "\n",
        "  predict_label = predicts[i][0][0]\n",
        "  color = get_label_color(predict_label,\n",
        "                          test_data.index_to_label[label.numpy()])\n",
        "  ax.xaxis.label.set_color(color)\n",
        "  plt.xlabel('Predicted: %s' % predict_label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3H0rkbLUZAG"
      },
      "source": [
        "Si la precisión no cumple los requisitos de la app, cabría remitirse al [Uso avanzado](#scrollTo=zNDBP2qA54aK) para buscar alternativas, como cambiar a un modelo más grande, ajustar los parámetros de reentrenamiento, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHoGAceO2xV"
      },
      "source": [
        "### Paso 4: Exporte a un modelo TensorFlow Lite\n",
        "\n",
        "Convierta el modelo entrenado al formato de modelo TensorFlow Lite con [metadatos](https://www.tensorflow.org/lite/models/convert/metadata) para poder usarlo posteriormente en una aplicación ML en el dispositivo. El archivo de etiquetas y el archivo de vocabulario están incrustados en los metadatos. El nombre de archivo TFLite predeterminado es `model.tflite`.\n",
        "\n",
        "En muchas aplicaciones de ML en el dispositivo, el tamaño del modelo es un factor importante. Por lo tanto, se recomienda aplicar la cuantización del modelo para hacerlo más pequeño y, potencialmente, ejecutarlo más rápido. La técnica de cuantización predeterminada tras el entrenamiento es la cuantización total para la tarea de clasificación de imágenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im6wA9lK3TQB"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROS2Ay2jMPCl"
      },
      "source": [
        "Vea la [guía de ejemplo](https://www.tensorflow.org/lite/examples/image_classification/overview) de clasificación de imágenes para saber más sobre cómo integrar el modelo TensorFlow Lite en apps móviles.\n",
        "\n",
        "Este modelo puede integrarse en una app Android o iOS usando la [API de Clasificador de imágenes](https://www.tensorflow.org/lite/inference_with_metadata/task_library/image_classifier) de la [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "habFnvRxxQ4A"
      },
      "source": [
        "Los formatos de exportación permitidos pueden ser uno o varios de los siguientes:\n",
        "\n",
        "- `ExportFormat.TFLITE`\n",
        "- `ExportFormat.LABEL`\n",
        "- `ExportFormat.SAVED_MODEL`\n",
        "\n",
        "De forma predeterminada, sólo exporta el modelo TensorFlow Lite con metadatos. También puede exportar selectivamente diferentes archivos. Por ejemplo, se puede exportar sólo el archivo de etiquetas de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvxWsOTmKG4P"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', export_format=ExportFormat.LABEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4jQaxyT5_KV"
      },
      "source": [
        "También puede evaluar el modelo tflite con el método `evaluate_tflite`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1YoPX5wOK-u"
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('model.tflite', test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNDBP2qA54aK"
      },
      "source": [
        "## Uso avanzado\n",
        "\n",
        "La función `create` es la parte crítica de esta librería. Usa el aprendizaje por transferencia con un modelo preentrenado similar al del [tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning).\n",
        "\n",
        "La función `create` contiene los siguientes pasos:\n",
        "\n",
        "1. Divida los datos en datos de entrenamiento, validación y prueba según los parámetros `validation_ratio` y `test_ratio`. El valor predeterminado de `validation_ratio` y `test_ratio` son `0.1` y `0.1`.\n",
        "2. Descargue un [Vector de características de imagen](https://www.tensorflow.org/hub/common_signatures/images#image_feature_vector) como modelo base desde TensorFlow Hub. El modelo predeterminado preentrenado es EfficientNet-Lite0.\n",
        "3. Añada una cabecera del clasificador con una capa Dropout con `dropout_rate` entre la capa de cabecera y el modelo preentrenado. El valor predeterminado `dropout_rate` es el valor predeterminado `dropout_rate` de [make_image_classifier_lib](https://github.com/tensorflow/hub/blob/master/tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py#L55) de TensorFlow Hub.\n",
        "4. Preprocesar los datos de entrada en bruto. Actualmente, los pasos del preprocesamiento incluyen normalizar el valor de cada pixel de la imagen para modelar la escala de entrada y redimensionarla para modelar el tamaño de entrada. EfficientNet-Lite0 tiene la escala de entrada `[0, 1]` y el tamaño de la imagen de entrada `[224, 224, 3]`.\n",
        "5. Introduzca los datos en el modelo clasificador. De forma predeterminada, los parámetros de entrenamiento, como épocas de entrenamiento, tamaño del lote, tasa de aprendizaje, impulso, son los valores predeterminados de [make_image_classifier_lib](https://github.com/tensorflow/hub/blob/master/tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py#L55) de TensorFlow Hub. Sólo se entrena la cabecera del clasificador.\n",
        "\n",
        "En esta sección describimos varios temas avanzados, como el cambio a un modelo de clasificación de imágenes diferente, la modificación de los hiperparámetros de entrenamiento, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc4Jk8TvBQfm"
      },
      "source": [
        "## Personalice la cuantización posterior al entrenamiento en el modelo TensorFLow Lite\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD8BOYrHBiDt"
      },
      "source": [
        "[La cuantización posterior al entrenamiento](https://www.tensorflow.org/lite/performance/post_training_quantization) es una técnica de conversión que puede reducir el tamaño del modelo y la latencia de la inferencia, al tiempo que mejora la velocidad de inferencia de la CPU y del acelerador de hardware, con una pequeña degradación de la precisión del modelo. Por ello, se usa mucho para optimizar el modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyIo0d5TCzE2"
      },
      "source": [
        "La librería Model Maker aplica una técnica predeterminada de cuantización postentrenamiento al exportar el modelo. Si desea personalizar la cuantización postentrenamiento, Model Maker también soporta múltiples opciones de cuantización postentrenamiento utilizando [QuantizationConfig](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/config/QuantizationConfig). Tomemos como ejemplo la cuantización float16. En primer lugar, defina la configuración de cuantización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8hL2mstCxQl"
      },
      "outputs": [],
      "source": [
        "config = QuantizationConfig.for_float16()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1gzx_rmFMOA"
      },
      "source": [
        "A continuación, exportamos el modelo TensorFlow Lite con dicha configuración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTJzFQnJFMjr"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Safo0e40wKZW"
      },
      "source": [
        "En Colab, puede descargar el modelo llamado `model_fp16.tflite` de la barra lateral izquierda, igual que la parte de carga mencionada anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4kiTJtZ_sDm"
      },
      "source": [
        "## Cambiar el modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "794vgj6ud7Ep"
      },
      "source": [
        "### Cambie al modelo compatible con esta librería.\n",
        "\n",
        "Esta librería ya es compatible con los modelos EfficientNet-Lite, MobileNetV2 y ResNet50. [EfficientNet-Lite](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite) son una familia de modelos de clasificación de imágenes que pueden alcanzar una precisión de última generación y son adecuados para dispositivos Edge. El modelo predeterminado es EfficientNet-Lite0.\n",
        "\n",
        "Podríamos cambiar de modelo a MobileNetV2 con sólo configurar el parámetro `model_spec` a la especificación del modelo MobileNetV2 en el método `create`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JKsJ6-P6ae1"
      },
      "outputs": [],
      "source": [
        "model = image_classifier.create(train_data, model_spec=model_spec.get('mobilenet_v2'), validation_data=validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm_B1Wv08AxR"
      },
      "source": [
        "Evalúe el modelo MobileNetV2 recién reentrenado para ver la precisión y la pérdida en los datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB2Go3HW8X7_"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAciGzVWtmWp"
      },
      "source": [
        "### Cambie al modelo en TensorFlow Hub\n",
        "\n",
        "Además, también podríamos pasar a otros modelos nuevos que introducen una imagen y emiten un vector de características con formato TensorFlow Hub.\n",
        "\n",
        "Tomando como ejemplo el modelo [Inception V3](https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1), podríamos definir `inception_v3_spec` que es un objeto de [image_classifier.ModelSpec](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/image_classifier/ModelSpec) y contiene la especificación del modelo Inception V3.\n",
        "\n",
        "Necesitamos especificar el nombre del modelo `name`, la url del modelo TensorFlow Hub `uri`. Mientras tanto, el valor predeterminado de `input_image_shape` es `[224, 224]`. Tenemos que cambiarlo a `[299, 299]` para el modelo Inception V3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdiMF2WMfAR4"
      },
      "outputs": [],
      "source": [
        "inception_v3_spec = image_classifier.ModelSpec(\n",
        "    uri='https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1')\n",
        "inception_v3_spec.input_image_shape = [299, 299]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_GGIoXZCs5F"
      },
      "source": [
        "Entonces, configurando el parámetro `model_spec` como `inception_v3_spec` en el método `create`, podríamos volver a entrenar el modelo Inception V3.\n",
        "\n",
        "Los pasos restantes son exactamente iguales y al final podremos tener un modelo InceptionV3 TensorFlow Lite personalizado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhZ5IRKdeex3"
      },
      "source": [
        "### Cambie su propio modelo personalizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svTjlZhrCrcV"
      },
      "source": [
        "Si queremos usar el modelo personalizado que no está en TensorFlow Hub, debemos crear y exportar [ModelSpec](https://www.tensorflow.org/hub/api_docs/python/hub/ModuleSpec) en TensorFlow Hub.\n",
        "\n",
        "A continuación, empiece a definir el objeto `ModelSpec` como en el proceso anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M9bn703AHt2"
      },
      "source": [
        "## Cambie los hiperparámetros de entrenamiento\n",
        "\n",
        "También podemos cambiar los hiperparámetros de entrenamiento como `epochs`, `dropout_rate` y `batch_size` que pueden afectar a la precisión del modelo. Los parámetros del modelo que puede ajustar son:\n",
        "\n",
        "- `epochs`: con más épocas se podría lograr una mayor precisión hasta que converja, pero un entrenamiento para demasiadas épocas puede llevar a un sobreajuste.\n",
        "- `dropout_rate`: La tasa de abandono, para evitar el sobreajuste. Ninguno, de forma predeterminada.\n",
        "- `batch_size`: Número de muestras a usar en un paso de entrenamiento. Ninguno, de forma predeterminada.\n",
        "- `validation_data`: Datos de validación. Si es Ninguno, omite el proceso de validación. Ninguno, de forma predeterminada.\n",
        "- `train_whole_model`: Si es true, el módulo Hub se entrena junto con la capa de clasificación superior. En caso contrario, sólo se entrena la capa de clasificación superior. Ninguno, de forma predeterminada.\n",
        "- `learning_rate`: Tasa de aprendizaje base. Ninguno, de forma predeterminada.\n",
        "- `momentum`: un float Python reenviado al optimizador. Sólo se usa cuando `use_hub_library` es True. Ninguno, de forma predeterminada.\n",
        "- `shuffle`: Booleano, si los datos deben mezclarse. False, de forma predeterminada.\n",
        "- `use_augmentation`: Booleano, use la aumentación de datos para el preprocesamiento. False, de forma predeterminada.\n",
        "- `use_hub_library`: Booleano, use `make_image_classifier_lib` de tensorflow hub para volver a entrenar el modelo. Esta canalización del entrenamiento podría lograr un mejor rendimiento para conjuntos de datos complicados con muchas categorías. True, de forma predeterminada.\n",
        "- `warmup_steps`: Número de pasos de calentamiento para el programa de calentamiento en la tasa de aprendizaje. Si es Ninguno, se usa el predeterminado warmup_steps que es el total de pasos de entrenamiento en dos épocas. Sólo se usa cuando `use_hub_library` es False. Ninguno, de forma predeterminada.\n",
        "- `model_dir`: Opcional, la ubicación de los archivos de punto de verificación del modelo. Sólo se usa cuando `use_hub_library` es False. Ninguno, de forma predeterminada.\n",
        "\n",
        "Los parámetros que son None de forma predeterminada como `epochs` recibirán los parámetros concretos predeterminados en [make_image_classifier_lib](https://github.com/tensorflow/hub/blob/02ab9b7d3455e99e97abecf43c5d598a5528e20c/tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py#L54) de la librería TensorFlow Hub o [train_image_classifier_lib](https://github.com/tensorflow/examples/blob/f0260433d133fd3cea4a920d1e53ecda07163aee/tensorflow_examples/lite/model_maker/core/task/train_image_classifier_lib.py#L61).\n",
        "\n",
        "Por ejemplo, podríamos entrenar con más épocas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3k7mhH54QcK"
      },
      "outputs": [],
      "source": [
        "model = image_classifier.create(train_data, validation_data=validation_data, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaYBQymQDsXU"
      },
      "source": [
        "Evalúe el modelo recién reentrenado con 10 épocas de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VafIYpKWD4Sw"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhBU5NCy5Ji2"
      },
      "source": [
        "# Más información\n",
        "\n",
        "Puede leer nuestro ejemplo de [clasificación de imágenes](https://www.tensorflow.org/lite/examples/image_classification/overview) para conocer los detalles técnicos. Para más información, consulte:\n",
        "\n",
        "- [Guía](https://www.tensorflow.org/lite/models/modify/model_maker) y [Referencia de API](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker) de Model Maker de TensorFlow Lite.\n",
        "- Librería de tareas: [ImageClassifier](https://www.tensorflow.org/lite/inference_with_metadata/task_library/image_classifier) para su implementación.\n",
        "- Las apps de referencia de principio a fin: [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android), [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/ios), y [Raspberry PI](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/raspberry_pi).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "image_classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
