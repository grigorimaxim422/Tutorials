{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLOXFOT5Q40E"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iiQkM5ZgQ8r2"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6331ZSsQGY3"
      },
      "source": [
        "# Clasificación MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9Jcnb8bQQyd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/quantum/tutorials/mnist\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/quantum/tutorials/mnist.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/quantum/tutorials/mnist.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/quantum/tutorials/mnist.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udLObUVeGfTs"
      },
      "source": [
        "En este tutorial creamos una red neuronal cuántica (QNN) para clasificar una versión simplificada de MNIST, similar al método utilizado en <a href=\"https://arxiv.org/pdf/1802.06002.pdf\" class=\"external\">Farhi et al</a>. Se compara el desempeño de una red neuronal cuántica con el de una clásica frente a un problema de datos clásico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X35qHdh5Gzqg"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TorxE5tnkvb2"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxkQA6oblNqI"
      },
      "source": [
        "Instalar TensorFlow Quantum:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saFHsRDpkvkH"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-quantum==0.7.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ql5PW-ACO0J"
      },
      "outputs": [],
      "source": [
        "# Update package resources to account for version changes.\n",
        "import importlib, pkg_resources\n",
        "importlib.reload(pkg_resources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdgMMZEBGqyl"
      },
      "source": [
        "Ahora, hay que importar TensorFlow y las dependencias del módulo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enZ300Bflq80"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "\n",
        "import cirq\n",
        "import sympy\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import collections\n",
        "\n",
        "# visualization tools\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from cirq.contrib.svg import SVGCircuit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b08Mmbs8lr81"
      },
      "source": [
        "## 1. Carga de los datos\n",
        "\n",
        "En este tutorial crearemos un clasificador binario para distinguir entre los dígitos 3 y 6, siguiendo lo expuesto por <a href=\"https://arxiv.org/pdf/1802.06002.pdf\" class=\"external\">Farhi et al.</a>. En esta sección se tratan los temas de manipulación de datos que incluye lo siguiente:\n",
        "\n",
        "- Cargar los datos sin procesar desde Keras.\n",
        "- Filtrar el conjunto de datos a solamente los 3 y los 6.\n",
        "- Reducir la escala de las imágenes para que quepan en una computadora cuántica.\n",
        "- Quitar cualquier ejemplo contradictorio.\n",
        "- Convertir las imágenes binarias a circuitos Cirq.\n",
        "- Convertir los circuitos Cirq a circuitos de TensorFlow Quantum. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDUdGxn-ojgy"
      },
      "source": [
        "### 1.1 Carga de los datos sin procesar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZyGXlaKojgz"
      },
      "source": [
        "Cargamos el conjunto de datos MNIST distribuido con Keras. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9OSExvCojg0"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
        "x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n",
        "\n",
        "print(\"Number of original training examples:\", len(x_train))\n",
        "print(\"Number of original test examples:\", len(x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZpbygdGojg3"
      },
      "source": [
        "Filtramos el conjunto de datos para conservar solamente los 3 y los 6, y eliminar las otras clases. Al mismo tiempo, convertimos la etiqueta `y` al booleano: `True` para `3` and `False` para 6. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOw68cCZojg4"
      },
      "outputs": [],
      "source": [
        "def filter_36(x, y):\n",
        "    keep = (y == 3) | (y == 6)\n",
        "    x, y = x[keep], y[keep]\n",
        "    y = y == 3\n",
        "    return x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-XEU8egGL6q"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = filter_36(x_train, y_train)\n",
        "x_test, y_test = filter_36(x_test, y_test)\n",
        "\n",
        "print(\"Number of filtered training examples:\", len(x_train))\n",
        "print(\"Number of filtered test examples:\", len(x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wyiaP0Xojg_"
      },
      "source": [
        "Mostramos el primer ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5STP7MbojhA"
      },
      "outputs": [],
      "source": [
        "print(y_train[0])\n",
        "\n",
        "plt.imshow(x_train[0, :, :, 0])\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNS9sVPQojhC"
      },
      "source": [
        "### 1.2 Reducción de las imágenes a escala"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmmtplIFGL6t"
      },
      "source": [
        "El tamaño de una imagen de 28×28 es demasiado grande para las computadores cuánticas actuales. Hay que cambiarles el tamaño a 4×4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbhUdBFWojhE"
      },
      "outputs": [],
      "source": [
        "x_train_small = tf.image.resize(x_train, (4,4)).numpy()\n",
        "x_test_small = tf.image.resize(x_test, (4,4)).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOMd7zIjGL6x"
      },
      "source": [
        "Nuevamente, mostramos el primer ejemplo de entrenamiento; después del cambio de tamaño: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIYOtCRIGL6y"
      },
      "outputs": [],
      "source": [
        "print(y_train[0])\n",
        "\n",
        "plt.imshow(x_train_small[0,:,:,0], vmin=0, vmax=1)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGeF1_qtojhK"
      },
      "source": [
        "### 1.3 Eliminación de los ejemplos contradictorios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZLkq2yeojhL"
      },
      "source": [
        "De la sección *3.3 Learning to Distinguish Digits* (Cómo aprender a distinguir dígitos) de <a href=\"https://arxiv.org/pdf/1802.06002.pdf\" class=\"external\">Farhi et al.</a>, filtre el conjunto de datos para quitar las imágenes etiquetadas como pertenecientes a ambas clases.\n",
        "\n",
        "No es un procedimiento de aprendizaje automático estándar, pero se incluye a fin de seguir lo expuesto en la publicación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqOPW0C7ojhL"
      },
      "outputs": [],
      "source": [
        "def remove_contradicting(xs, ys):\n",
        "    mapping = collections.defaultdict(set)\n",
        "    orig_x = {}\n",
        "    # Determine the set of labels for each unique image:\n",
        "    for x,y in zip(xs,ys):\n",
        "       orig_x[tuple(x.flatten())] = x\n",
        "       mapping[tuple(x.flatten())].add(y)\n",
        "    \n",
        "    new_x = []\n",
        "    new_y = []\n",
        "    for flatten_x in mapping:\n",
        "      x = orig_x[flatten_x]\n",
        "      labels = mapping[flatten_x]\n",
        "      if len(labels) == 1:\n",
        "          new_x.append(x)\n",
        "          new_y.append(next(iter(labels)))\n",
        "      else:\n",
        "          # Throw out images that match more than one label.\n",
        "          pass\n",
        "    \n",
        "    num_uniq_3 = sum(1 for value in mapping.values() if len(value) == 1 and True in value)\n",
        "    num_uniq_6 = sum(1 for value in mapping.values() if len(value) == 1 and False in value)\n",
        "    num_uniq_both = sum(1 for value in mapping.values() if len(value) == 2)\n",
        "\n",
        "    print(\"Number of unique images:\", len(mapping.values()))\n",
        "    print(\"Number of unique 3s: \", num_uniq_3)\n",
        "    print(\"Number of unique 6s: \", num_uniq_6)\n",
        "    print(\"Number of unique contradicting labels (both 3 and 6): \", num_uniq_both)\n",
        "    print()\n",
        "    print(\"Initial number of images: \", len(xs))\n",
        "    print(\"Remaining non-contradicting unique images: \", len(new_x))\n",
        "    \n",
        "    return np.array(new_x), np.array(new_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMOiJfz_ojhP"
      },
      "source": [
        "Los resultados de los conteos no coinciden con los valores informados, pero no se ha especificado un procedimiento exacto.\n",
        "\n",
        "Tampoco importa, en este caso, que la aplicación de filtros a ejemplos contradictorios en este punto no evite por completo que el modelo reciba ejemplos contradictorios de entrenamiento: en el paso siguiente se binarizan los datos que causarán más colisiones. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpnsAssWojhP"
      },
      "outputs": [],
      "source": [
        "x_train_nocon, y_train_nocon = remove_contradicting(x_train_small, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlJ5NVaPojhT"
      },
      "source": [
        "### 1.4 Codificación de los datos como circuitos cuánticos\n",
        "\n",
        "Para procesar las imágenes con una computadora cuántica, <a href=\"https://arxiv.org/pdf/1802.06002.pdf\" class=\"external\">Farhi et al.</a> propusieron representar cada pixel con un bit cuántico, con el estado pendiente con respecto al valor del pixel. El primer paso es convertir la codificación binaria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z8J7OyDojhV"
      },
      "outputs": [],
      "source": [
        "THRESHOLD = 0.5\n",
        "\n",
        "x_train_bin = np.array(x_train_nocon > THRESHOLD, dtype=np.float32)\n",
        "x_test_bin = np.array(x_test_small > THRESHOLD, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlJ5NVaPojhU"
      },
      "source": [
        "Si quitáramos las imágenes contradictorias en este punto, solamente quedarían 193 y, probablemente, no serían suficientes para hacer un entrenamiento efectivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z8J7OyDojhW"
      },
      "outputs": [],
      "source": [
        "_ = remove_contradicting(x_train_bin, y_train_nocon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLyxS9KlojhZ"
      },
      "source": [
        "Los bits cuánticos de los índices del pixel con valores que exceden el límite se rotan a través de una puerta $X$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOu_3-3ZGL61"
      },
      "outputs": [],
      "source": [
        "def convert_to_circuit(image):\n",
        "    \"\"\"Encode truncated classical image into quantum datapoint.\"\"\"\n",
        "    values = np.ndarray.flatten(image)\n",
        "    qubits = cirq.GridQubit.rect(4, 4)\n",
        "    circuit = cirq.Circuit()\n",
        "    for i, value in enumerate(values):\n",
        "        if value:\n",
        "            circuit.append(cirq.X(qubits[i]))\n",
        "    return circuit\n",
        "\n",
        "\n",
        "x_train_circ = [convert_to_circuit(x) for x in x_train_bin]\n",
        "x_test_circ = [convert_to_circuit(x) for x in x_test_bin]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSCXqzOzojhd"
      },
      "source": [
        "El siguiente es el circuito creado para el primer ejemplo (en los diagramas del circuito no se muestran los bits cuánticos con puertas en cero):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3POmUEUojhe"
      },
      "outputs": [],
      "source": [
        "SVGCircuit(x_train_circ[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEQMxCcBojhg"
      },
      "source": [
        "Comparamos este circuito con los índices en los que el valor de la imagen excede al límite:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBIsiXdtojhh"
      },
      "outputs": [],
      "source": [
        "bin_img = x_train_bin[0,:,:,0]\n",
        "indices = np.array(np.where(bin_img)).T\n",
        "indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWZ24w1Oojhk"
      },
      "source": [
        "Convertimos estos circuitos `Cirq` a tensores para `tfq`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZStEMk4ojhk"
      },
      "outputs": [],
      "source": [
        "x_train_tfcirc = tfq.convert_to_tensor(x_train_circ)\n",
        "x_test_tfcirc = tfq.convert_to_tensor(x_test_circ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4USiqeOqGL67"
      },
      "source": [
        "## 2. Red neuronal cuántica\n",
        "\n",
        "Hay poca información relacionada con la estructura de los circuitos cuánticos que clasifican imágenes. Dado que la clasificación se basa en la esperanza del bit cuántico que se lee, <a href=\"https://arxiv.org/pdf/1802.06002.pdf\" class=\"external\">Farhi et al.</a> proponen utilizar dos puertas de bit cuántico con el cúbit leído sobre el que siempre nos basamos. Es parecido, en cierto modo, a ejecutar en una pequeña proporción una <a href=\"https://arxiv.org/abs/1511.06464\" class=\"external\">red neuronal recurrente unitaria</a> a través de los pixeles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knIzawEeojho"
      },
      "source": [
        "### 2.1 Creación del circuito modelo\n",
        "\n",
        "En el siguiente ejemplo se muestra esta aproximación por capas. Cada capa usa *n* instancias de la misma puerta, con cada uno de los bits cuánticos de datos que actúan sobre le bit cuántico leído.\n",
        "\n",
        "Empezamos con una clase simple que agregará una capa de estas puertas a un circuito:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hjxxgU5ojho"
      },
      "outputs": [],
      "source": [
        "class CircuitLayerBuilder():\n",
        "    def __init__(self, data_qubits, readout):\n",
        "        self.data_qubits = data_qubits\n",
        "        self.readout = readout\n",
        "    \n",
        "    def add_layer(self, circuit, gate, prefix):\n",
        "        for i, qubit in enumerate(self.data_qubits):\n",
        "            symbol = sympy.Symbol(prefix + '-' + str(i))\n",
        "            circuit.append(gate(qubit, self.readout)**symbol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjo5hANFojhr"
      },
      "source": [
        "Creamos una capa del circuito de ejemplo para ver cómo queda:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzXWOpUGojhs"
      },
      "outputs": [],
      "source": [
        "demo_builder = CircuitLayerBuilder(data_qubits = cirq.GridQubit.rect(4,1),\n",
        "                                   readout=cirq.GridQubit(-1,-1))\n",
        "\n",
        "circuit = cirq.Circuit()\n",
        "demo_builder.add_layer(circuit, gate = cirq.XX, prefix='xx')\n",
        "SVGCircuit(circuit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-QhPE1pojhu"
      },
      "source": [
        "Ahora, construimos un modelo con dos capas, que coincida con el tamaño del circuito y los datos, e incluimos las operaciones de preparación y lectura."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiALbpwRGL69"
      },
      "outputs": [],
      "source": [
        "def create_quantum_model():\n",
        "    \"\"\"Create a QNN model circuit and readout operation to go along with it.\"\"\"\n",
        "    data_qubits = cirq.GridQubit.rect(4, 4)  # a 4x4 grid.\n",
        "    readout = cirq.GridQubit(-1, -1)         # a single qubit at [-1,-1]\n",
        "    circuit = cirq.Circuit()\n",
        "    \n",
        "    # Prepare the readout qubit.\n",
        "    circuit.append(cirq.X(readout))\n",
        "    circuit.append(cirq.H(readout))\n",
        "    \n",
        "    builder = CircuitLayerBuilder(\n",
        "        data_qubits = data_qubits,\n",
        "        readout=readout)\n",
        "\n",
        "    # Then add layers (experiment by adding more).\n",
        "    builder.add_layer(circuit, cirq.XX, \"xx1\")\n",
        "    builder.add_layer(circuit, cirq.ZZ, \"zz1\")\n",
        "\n",
        "    # Finally, prepare the readout qubit.\n",
        "    circuit.append(cirq.H(readout))\n",
        "\n",
        "    return circuit, cirq.Z(readout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QZvVh7vojhx"
      },
      "outputs": [],
      "source": [
        "model_circuit, model_readout = create_quantum_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY7vbY6yfABE"
      },
      "source": [
        "### 2.2 Encapsulamiento de un circuito modelo en un modelo tfq-keras\n",
        "\n",
        "Creamos el modelo Keras con componentes cuánticos. Este modelo se alimenta con \"datos cuánticos\", de `x_train_circ`, que codifican los datos clásicos. Usa una capa del *circuito cuántico parametrizado*, `tfq.layers.PQC`, para entrenar el circuito del modelo sobre los datos cuánticos.\n",
        "\n",
        "Para clasificar estas imágenes, <a href=\"https://arxiv.org/pdf/1802.06002.pdf\" class=\"external\">Farhi et al.</a> propuso tomar la esperanza de un bit cuántico de lectura en un circuito parametrizado. La esperanza devuelve un valor entre 1 y -1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYdf_KOxojh0"
      },
      "outputs": [],
      "source": [
        "# Build the Keras model.\n",
        "model = tf.keras.Sequential([\n",
        "    # The input is the data-circuit, encoded as a tf.string\n",
        "    tf.keras.layers.Input(shape=(), dtype=tf.string),\n",
        "    # The PQC layer returns the expected value of the readout gate, range [-1,1].\n",
        "    tfq.layers.PQC(model_circuit, model_readout),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz-FbVc9ojh3"
      },
      "source": [
        "A continuación, describimos el procedimiento de entrenamiento para el modelo, con el método `compile`.\n",
        "\n",
        "Como la lectura esperada se encuentra en el rango de `[-1,1]`, la optimización de la pérdida de articulación es, en cierto modo, natural.\n",
        "\n",
        "Nota: Otra opción válida sería cambiar el rango de salida a `[0,1]` y tratarlo como la probabilidad que le asigna el modelo a la clase `3`. Se podría usar con un estándar, una pérdida `tf.losses.BinaryCrossentropy`.\n",
        "\n",
        "Para usar la pérdida de articulación es necesario hacer dos ajustes pequeños. Primero debemos convertir las etiquetas `y_train_nocon`, de booleanas a `[-1,1]`, tal como se espera para la pérdida de articulación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgMNkC1Fojh5"
      },
      "outputs": [],
      "source": [
        "y_train_hinge = 2.0*y_train_nocon-1.0\n",
        "y_test_hinge = 2.0*y_test-1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nwnveDiojh7"
      },
      "source": [
        "En segundo lugar, usaremos una métrica de `hinge_accuracy` personalizada que administra correctamente `[-1, 1]` como el argumento de las etiquetas `y_true`. `tf.losses.BinaryAccuracy(threshold=0.0)` espera que `y_true` sea booleano y, por lo tanto, no se puede usar con la pérdida de articulación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XKtZ_TEojh8"
      },
      "outputs": [],
      "source": [
        "def hinge_accuracy(y_true, y_pred):\n",
        "    y_true = tf.squeeze(y_true) > 0.0\n",
        "    y_pred = tf.squeeze(y_pred) > 0.0\n",
        "    result = tf.cast(y_true == y_pred, tf.float32)\n",
        "\n",
        "    return tf.reduce_mean(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlpETlLRojiA"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.Hinge(),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[hinge_accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkHq2RstojiC"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsuOzDYblA9s"
      },
      "source": [
        "### Entrenamiento del modelo cuántico\n",
        "\n",
        "Ahora, entrenemos el modelo. Tomará alrededor de 45 minutos. Si no queremos esperar tanto tiempo, podemos usar un subconjunto pequeño de datos (establecer `NUM_EXAMPLES=500`, como a continuación). Realmente no afectará al progreso del modelo durante el entrenamiento (solamente tiene 32 parámetros y no necesita muchos datos que los restrinjan). Con menos ejemplos, simplemente, termina antes (en 5 minutos), pero se ejecuta el tiempo suficiente como para mostrar que se producen avances con los registros de validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8vuQpSLlBV2"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 3\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "NUM_EXAMPLES = len(x_train_tfcirc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJnNG-3JojiI"
      },
      "outputs": [],
      "source": [
        "x_train_tfcirc_sub = x_train_tfcirc[:NUM_EXAMPLES]\n",
        "y_train_hinge_sub = y_train_hinge[:NUM_EXAMPLES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMSdgGC1GL7D"
      },
      "source": [
        "El entrenamiento de este modelo para la convergencia debería alcanzar &gt;85% de precisión en el conjunto de la prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya9qP3KkojiM"
      },
      "outputs": [],
      "source": [
        "qnn_history = model.fit(\n",
        "      x_train_tfcirc_sub, y_train_hinge_sub,\n",
        "      batch_size=32,\n",
        "      epochs=EPOCHS,\n",
        "      verbose=1,\n",
        "      validation_data=(x_test_tfcirc, y_test_hinge))\n",
        "\n",
        "qnn_results = model.evaluate(x_test_tfcirc, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ER7B7aaojiP"
      },
      "source": [
        "Nota: La exactitud del entrenamiento presenta el promedio sobre la época. La exactitud de la validación se evalúa al final de cada época."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8952YvuWGL7J"
      },
      "source": [
        "## 3. Red neuronal clásica\n",
        "\n",
        "Mientras que la red neuronal cuántica funciona bien con este problema de MNIST simplificado, una red neuronal clásica, fácilmente, puede tener un mejor desempeño que una QNN en esta tarea. Después de una sola época, la red neuronal clásica puede lograr &gt;98% de exactitud con el conjunto retenido.\n",
        "\n",
        "En el siguiente ejemplo, se usa una red neuronal clásica para el problema de clasificación de 3-6 con una imagen entera de 28×28, en vez de utilizar una submuestra de la imagen. Fácilmente converge en aproximadamente un 100% de exactitud del conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZofEHhLGL7L"
      },
      "outputs": [],
      "source": [
        "def create_classical_model():\n",
        "    # A simple model based off LeNet from https://keras.io/examples/mnist_cnn/\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, [3, 3], activation='relu', input_shape=(28,28,1)))\n",
        "    model.add(tf.keras.layers.Conv2D(64, [3, 3], activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    return model\n",
        "\n",
        "\n",
        "model = create_classical_model()\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiAJl7sZojiU"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          batch_size=128,\n",
        "          epochs=1,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "cnn_results = model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5-5BVJaojiZ"
      },
      "source": [
        "Este modelo anterior tiene aproximadamente 1 200 000 parámetros. Para que la comparación resulte más justa, probemos con un modelo de 37 parámetros, con las submuestras de las imágenes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70TOM6r-ojiZ"
      },
      "outputs": [],
      "source": [
        "def create_fair_classical_model():\n",
        "    # A simple model based off LeNet from https://keras.io/examples/mnist_cnn/\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten(input_shape=(4,4,1)))\n",
        "    model.add(tf.keras.layers.Dense(2, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    return model\n",
        "\n",
        "\n",
        "model = create_fair_classical_model()\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA_Fx-8gojid"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train_bin,\n",
        "          y_train_nocon,\n",
        "          batch_size=128,\n",
        "          epochs=20,\n",
        "          verbose=2,\n",
        "          validation_data=(x_test_bin, y_test))\n",
        "\n",
        "fair_nn_results = model.evaluate(x_test_bin, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH3mam7EGL7N"
      },
      "source": [
        "## 4. Comparación\n",
        "\n",
        "Mientras más alta sea la entrada de la resolución y más potente sea el modelo, el problema se hará más sencillo para la CNN. Mientras que un modelo clásico de potencia similar (con ~32 parámetros) entrena hasta lograr una exactitud similar en una fracción de tiempo. De un modo u otro, la red neuronal clásica tiene un mejor desempeño que la red neuronal cuántica. Para los datos clásicos, es difícil ganarle a una red neuronal clásica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOMeN7pMGL7P"
      },
      "outputs": [],
      "source": [
        "qnn_accuracy = qnn_results[1]\n",
        "cnn_accuracy = cnn_results[1]\n",
        "fair_nn_accuracy = fair_nn_results[1]\n",
        "\n",
        "sns.barplot(x=[\"Quantum\", \"Classical, full\", \"Classical, fair\"],\n",
        "            y=[qnn_accuracy, cnn_accuracy, fair_nn_accuracy])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mnist.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
