{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2J3nB-ZrRv1"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Probability Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9qDhTJmprPnm"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfPtIQ3DdZ8r"
      },
      "source": [
        "# Modelos lineales generalizados\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/probability/examples/Generalized_Linear_Models\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/probability/examples/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/probability/examples/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/probability/examples/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOfH1_F9YsOG"
      },
      "source": [
        "En este bloc de notas, se presentan los Modelos Lineales Generalizados (GLM) a través de un ejemplo resuelto. Este ejemplo se resuelve de dos maneras diferentes con dos algoritmos para ajustar los GLM de manera eficiente en TensorFlow Probability: puntuación de Fisher para datos densos y descenso de gradiente proximal en coordenadas para datos dispersos. Se comparan los coeficientes ajustados con los coeficientes verdaderos y, en el caso de un descenso de gradiente proximal en coordenadas, con la salida del algoritmo `glmnet` similar de R. Finalmente, se ofrecen más detalles matemáticos y derivaciones de varias propiedades clave de los GLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjsfQ6vLb5I0"
      },
      "source": [
        "# Antecedentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdMX-QKagFnY"
      },
      "source": [
        "Un modelo lineal generalizado (GLM) es un modelo lineal ($\\eta = x^\\top \\beta$) envuelto en una transformación (función de enlace) y equipado con una distribución de respuesta de una familia exponencial. La elección de la función de enlace y la distribución de la respuesta es muy flexible, lo que otorga una gran expresividad a los GLM. Los detalles completos, incluida una presentación secuencial de todas las definiciones y resultados de los GLM en notación inequívoca, se encuentran en \"Derivación de hechos del GLM\" a continuación. Resumimos:\n",
        "\n",
        "En un GLM, una distribución predictiva para la variable de respuesta $Y$ se asocia con un vector de predictores observados $x$. La distribución tiene la siguiente forma:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  p(y \\, |\\, x)\n",
        "&=\n",
        "  m(y, \\phi) \\exp\\left(\\frac{\\theta\\, T(y) - A(\\theta)}{\\phi}\\right)\n",
        "\\\\\n",
        "  \\theta\n",
        "&:=\n",
        "  h(\\eta)\n",
        "\\\\\n",
        "  \\eta\n",
        "&:=\n",
        "  x^\\top \\beta\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Aquí $\\beta$ son los parámetros (\"ponderaciones\"), $\\phi$ un hiperparámetro que representa la dispersión (\"varianza\") y $m$, $h$, $T$, $A$ se caracterizan por la familia de modelos especificada por el usuario.\n",
        "\n",
        "La media de $Y$ depende de $x$ por la composición de **la respuesta lineal** $\\eta$ y la función de enlace (inversa), es decir:\n",
        "\n",
        "$$\n",
        "\\mu := g^{-1}(\\eta)\n",
        "$$\n",
        "\n",
        "donde $g$ es la conocida **función de enlace**. En TFP, la elección de la función de enlace y la familia de modelos se especifican conjuntamente mediante una subclase `tfp.glm.ExponentialFamily`. Entre los ejemplos, se incluyen:\n",
        "\n",
        "- `tfp.glm.Normal`, también conocido como \"regresión lineal\"\n",
        "- `tfp.glm.Bernoulli`, también conocido como \"regresión logística\"\n",
        "- `tfp.glm.Poisson`, también conocido como \"regresión de Poisson\"\n",
        "- `tfp.glm.BernoulliNormalCDF`, también conocido como \"regresión probit\".\n",
        "\n",
        "TFP prefiere nombrar familias modelo según la distribución sobre `Y` en lugar de la función de enlace, ya que las distribuciones `tfp.Distribution` ya son ciudadanos de primera clase. Si el nombre de la subclase `tfp.glm.ExponentialFamily` contiene una segunda palabra, esto indica una [función de enlace no canónica](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oGScpRnqH_b"
      },
      "source": [
        "Los GLM tienen varias propiedades notables que permiten la implementación eficiente del estimador de máxima verosimilitud. Entre estas propiedades se destacan las fórmulas simples para el gradiente de la probabilidad logarítmica $\\ell$ y para la matriz de información de Fisher, que es el valor esperado del hessiano de la probabilidad logarítmica negativa bajo un nuevo muestreo de la respuesta bajo los mismos predictores. Es decir:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\nabla_\\beta\\, \\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{y})\n",
        "&=\n",
        "  \\mathbf{x}^\\top\n",
        "  \\,\\text{diag}\\left(\\frac{\n",
        "      {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)\n",
        "    }{\n",
        "      {\\textbf{Var}_T}(\\mathbf{x} \\beta)\n",
        "    }\\right)\n",
        "  \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta)\\right)\n",
        "\\\\\n",
        "  \\mathbb{E}_{Y_i \\sim \\text{GLM} | x_i} \\left[\n",
        "    \\nabla_\\beta^2\\, \\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{Y})\n",
        "  \\right]\n",
        "&=\n",
        "  -\\mathbf{x}^\\top\n",
        "  \\,\\text{diag}\\left(\n",
        "    \\frac{\n",
        "      \\phi\\, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)^2\n",
        "    }{\n",
        "      {\\textbf{Var}_T}(\\mathbf{x} \\beta)\n",
        "    }\\right)\\,\n",
        "  \\mathbf{x}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "donde $\\mathbf{x}$ es la matriz cuya $i$ésima fila es el vector predictor para la $i$ésima muestra de datos, y $\\mathbf{y}$ es el vector cuya $i$ésima coordenada es la respuesta observada para la $i$ésima muestra de datos. Aquí (en términos generales), ${\\text{Mean}_T}(\\eta) := \\mathbb{E}[T(Y),|,\\eta]$ y ${\\text{Var}_T}(\\ eta) := \\text{Var}[T(Y),|,\\eta]$, y la negrita denota vectorización de estas funciones. Todos los detalles de lo que representan las distribuciones de estas expectativas y varianzas se pueden encontrar en \"Derivación de hechos de GLM\" a continuación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuNDwfwBObKl"
      },
      "source": [
        "# Un ejemplo\n",
        "\n",
        "En esta sección, describimos y mostramos brevemente dos algoritmos de ajuste de GLM integrados en TensorFlow Probability: puntuación de Fisher (`tfp.glm.fit`) y descenso de gradiente proximal por coordenadas (`tfp.glm.fit_sparse`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4phryMfsP4Sn"
      },
      "source": [
        "## Conjunto de datos sintéticos\n",
        "\n",
        "Supongamos que cargamos algún conjunto de datos de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA2Rf9PPgMAD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEVnTz2hh9RN"
      },
      "outputs": [],
      "source": [
        "def make_dataset(n, d, link, scale=1., dtype=np.float32):\n",
        "  model_coefficients = tfd.Uniform(\n",
        "      low=-1., high=np.array(1, dtype)).sample(d, seed=42)\n",
        "  radius = np.sqrt(2.)\n",
        "  model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n",
        "  mask = tf.random.shuffle(tf.range(d)) < int(0.5 * d)\n",
        "  model_coefficients = tf.where(\n",
        "      mask, model_coefficients, np.array(0., dtype))\n",
        "  model_matrix = tfd.Normal(\n",
        "      loc=0., scale=np.array(1, dtype)).sample([n, d], seed=43)\n",
        "  scale = tf.convert_to_tensor(scale, dtype)\n",
        "  linear_response = tf.linalg.matvec(model_matrix, model_coefficients)\n",
        "  \n",
        "  if link == 'linear':\n",
        "    response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n",
        "  elif link == 'probit':\n",
        "    response = tf.cast(\n",
        "        tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n",
        "                   dtype)\n",
        "  elif link == 'logit':\n",
        "    response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n",
        "  else:\n",
        "    raise ValueError('unrecognized true link: {}'.format(link))\n",
        "  return model_matrix, response, model_coefficients, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99Fk5XZKbvi4"
      },
      "source": [
        "### Nota: Conéctese a un entorno de ejecución local.\n",
        "\n",
        "En este bloc de notas, compartimos datos entre los núcleos de Python y R a través de archivos locales. Para habilitar este uso compartido, utilice tiempos de ejecución en la misma máquina donde tiene permiso para leer y escribir archivos locales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EAQjTrZJqKx"
      },
      "outputs": [],
      "source": [
        "x, y, model_coefficients_true, _ = [t.numpy() for t in make_dataset(\n",
        "    n=int(1e5), d=100, link='probit')]\n",
        "\n",
        "DATA_DIR = '/tmp/glm_example'\n",
        "tf.io.gfile.makedirs(DATA_DIR)\n",
        "with tf.io.gfile.GFile('{}/x.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, x, delimiter=',')\n",
        "with tf.io.gfile.GFile('{}/y.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, y.astype(np.int32) + 1, delimiter=',', fmt='%d')\n",
        "with tf.io.gfile.GFile(\n",
        "    '{}/model_coefficients_true.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, model_coefficients_true, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P5I-aJdN6GZ"
      },
      "source": [
        "## Sin regularización L1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN6HfiH3bAb0"
      },
      "source": [
        "La función `tfp.glm.fit` implementa la puntuación de Fisher, que toma como uno de sus argumentos:\n",
        "\n",
        "- `model_matrix` = $\\mathbf{x}$\n",
        "- `response` = $\\mathbf{y}$\n",
        "- `model` = invocable que, dado el argumento $\\boldsymbol{\\eta}$, devuelve el triple $\\left(\n",
        "{\\textbf{Mean}_T}(\\boldsymbol{\\eta}),\n",
        "{\\textbf{Var}_T}(\\boldsymbol{\\eta}),\n",
        "{\\textbf{Mean}_T}'(\\boldsymbol{\\eta})\n",
        "\\right)$.\n",
        "\n",
        "Recomendamos que `model` sea una instancia de la clase `tfp.glm.ExponentialFamily`. Hay varias implementaciones prediseñadas disponibles, por lo que para los GLM más comunes no se requiere ningún código personalizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXkxVBSmesjn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_converged: True\n",
            "    num_iter: 6\n",
            "    accuracy: 0.75241\n",
            "    deviance: -0.992436110973\n",
            "||w0-w1||_2 / (1+||w0||_2): 0.0231555201462\n"
          ]
        }
      ],
      "source": [
        "@tf.function(autograph=False)\n",
        "def fit_model():\n",
        "  model_coefficients, linear_response, is_converged, num_iter = tfp.glm.fit(\n",
        "      model_matrix=x, response=y, model=tfp.glm.BernoulliNormalCDF())\n",
        "  log_likelihood = tfp.glm.BernoulliNormalCDF().log_prob(y, linear_response)\n",
        "  return (model_coefficients, linear_response, is_converged, num_iter,\n",
        "          log_likelihood)\n",
        " \n",
        "[model_coefficients, linear_response, is_converged, num_iter,\n",
        " log_likelihood] = [t.numpy() for t in fit_model()]\n",
        "\n",
        "print(('is_converged: {}\\n'\n",
        "       '    num_iter: {}\\n'\n",
        "       '    accuracy: {}\\n'\n",
        "       '    deviance: {}\\n'\n",
        "       '||w0-w1||_2 / (1+||w0||_2): {}'\n",
        "      ).format(\n",
        "    is_converged,\n",
        "    num_iter,\n",
        "    np.mean((linear_response > 0.) == y),\n",
        "    2. * np.mean(log_likelihood),\n",
        "    np.linalg.norm(model_coefficients_true - model_coefficients, ord=2) /\n",
        "        (1. + np.linalg.norm(model_coefficients_true, ord=2))\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6qexoHAJzEF"
      },
      "source": [
        "### Detalles matemáticos\n",
        "\n",
        "La puntuación de Fisher es una modificación del método de Newton que nos permite encontrar la estimación de máxima verosimilitud\n",
        "\n",
        "$$\n",
        "\\hat\\beta\n",
        ":= \\underset{\\beta}{\\text{arg max}}\\ \\ \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y}).\n",
        "$$\n",
        "\n",
        "El método de Newton básico, que busca ceros en el gradiente del logaritmo de verosimilitud, seguiría la regla de actualización\n",
        "\n",
        "$$\n",
        "  \\beta^{(t+1)}_{\\text{Newton}}\n",
        ":=\n",
        "  \\beta^{(t)}\n",
        "  -\n",
        "  \\alpha\n",
        "  \\left(\n",
        "    \\nabla^2_\\beta\\, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y})\n",
        "  \\right)_{\\beta = \\beta^{(t)}}^{-1}\n",
        "  \\left(\n",
        "    \\nabla_\\beta\\, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y})\n",
        "  \\right)_{\\beta = \\beta^{(t)}}\n",
        "$$\n",
        "\n",
        "donde $\\alpha \\in (0, 1]$ es una tasa de aprendizaje que se usa para controlar el tamaño del paso.\n",
        "\n",
        "En la puntuación de Fisher, el hessiano se reemplaza por la matriz de información negativa de Fisher:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\beta^{(t+1)}\n",
        "&:=\n",
        "  \\beta^{(t)}\n",
        "  -\n",
        "  \\alpha\\,\n",
        "  \\mathbb{E}_{\n",
        "    Y_i \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta^{(t)}), \\phi)\n",
        "  }\n",
        "  \\left[\n",
        "    \\left(\n",
        "      \\nabla^2_\\beta\\, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{Y})\n",
        "    \\right)_{\\beta = \\beta^{(t)}}\n",
        "  \\right]^{-1}\n",
        "  \\left(\n",
        "    \\nabla_\\beta\\, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y})\n",
        "  \\right)_{\\beta = \\beta^{(t)}} \\\\[3mm]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "[Tenga en cuenta que aquí $\\mathbf{Y} = (Y_i)_{i=1}^{n}$ es aleatorio, mientras que $\\mathbf{y}$ sigue siendo el vector de respuestas observadas].\n",
        "\n",
        "Mediante las fórmulas en \"Ajuste de parámetros del GLM a los datos\" a continuación, esto se simplifica de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\beta^{(t+1)}\n",
        "&=\n",
        "  \\beta^{(t)}\n",
        "  +\n",
        "  \\alpha\n",
        "  \\left(\n",
        "    \\mathbf{x}^\\top\n",
        "    \\text{diag}\\left(\n",
        "      \\frac{\n",
        "        \\phi\\, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t)})^2\n",
        "      }{\n",
        "        {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t)})\n",
        "      }\\right)\\,\n",
        "    \\mathbf{x}\n",
        "  \\right)^{-1}\n",
        "  \\left(\n",
        "    \\mathbf{x}^\\top\n",
        "    \\text{diag}\\left(\\frac{\n",
        "        {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t)})\n",
        "      }{\n",
        "        {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t)})\n",
        "      }\\right)\n",
        "    \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta^{(t)})\\right)\n",
        "  \\right).\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076quM7tN8_1"
      },
      "source": [
        "## Con regularización L1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnP3jeZOk7Y5"
      },
      "source": [
        "`tfp.glm.fit_sparse` implementa un ajustador del GLM más adecuado para conjuntos de datos dispersos, basado en el algoritmo de [Yuan, Ho y Lin 2012](#1). Sus características incluyen lo que sigue:\n",
        "\n",
        "- Regularización L1\n",
        "- Sin inversiones de matrices\n",
        "- Pocas evaluaciones del gradiente y del hessiano\n",
        "\n",
        "En primer lugar, presentamos un ejemplo de uso del código. Los detalles del algoritmo se detallan mejor en \"Detalles del algoritmo para `tfp.glm.fit_sparse`\" a continuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Oky1X4ijfv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_converged: True\n",
            "    num_iter: 1\n",
            "\n",
            "Coefficients:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Learned</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.216240</td>\n",
              "      <td>0.220758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.043702</td>\n",
              "      <td>0.063950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.145379</td>\n",
              "      <td>-0.153256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.024382</td>\n",
              "      <td>0.046572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.242985</td>\n",
              "      <td>-0.242609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.106168</td>\n",
              "      <td>-0.123367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.039745</td>\n",
              "      <td>-0.067560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.217717</td>\n",
              "      <td>-0.222169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.016553</td>\n",
              "      <td>-0.041692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.018959</td>\n",
              "      <td>0.049624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.057686</td>\n",
              "      <td>-0.078299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.003642</td>\n",
              "      <td>0.035682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.234406</td>\n",
              "      <td>-0.240482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.232209</td>\n",
              "      <td>0.225448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.130166</td>\n",
              "      <td>0.144485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>-0.178534</td>\n",
              "      <td>-0.186722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.218493</td>\n",
              "      <td>0.229656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.195579</td>\n",
              "      <td>0.200442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.031153</td>\n",
              "      <td>0.050457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.229065</td>\n",
              "      <td>0.231451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>-0.006512</td>\n",
              "      <td>-0.039516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-0.107947</td>\n",
              "      <td>-0.119896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.149419</td>\n",
              "      <td>0.171693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.047955</td>\n",
              "      <td>0.063434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.083171</td>\n",
              "      <td>-0.107145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.084615</td>\n",
              "      <td>0.101221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>-0.168431</td>\n",
              "      <td>-0.175473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.138411</td>\n",
              "      <td>0.152623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.061161</td>\n",
              "      <td>0.081945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>-0.083348</td>\n",
              "      <td>-0.104929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-0.141154</td>\n",
              "      <td>-0.153871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Learned      True\n",
              "0   0.216240  0.220758\n",
              "1   0.000000  0.000000\n",
              "2   0.000000  0.000000\n",
              "3   0.000000  0.000000\n",
              "4   0.000000  0.000000\n",
              "5   0.043702  0.063950\n",
              "6  -0.145379 -0.153256\n",
              "7   0.000000  0.000000\n",
              "8   0.000000  0.000000\n",
              "9   0.000000  0.000000\n",
              "10  0.000000  0.000000\n",
              "11  0.000000  0.000000\n",
              "12  0.000000  0.000000\n",
              "13  0.024382  0.046572\n",
              "14 -0.242985 -0.242609\n",
              "15 -0.106168 -0.123367\n",
              "16  0.000000  0.000000\n",
              "17 -0.039745 -0.067560\n",
              "18 -0.217717 -0.222169\n",
              "19  0.000000  0.000000\n",
              "20  0.000000  0.000000\n",
              "21 -0.016553 -0.041692\n",
              "22  0.018959  0.049624\n",
              "23 -0.057686 -0.078299\n",
              "24  0.003642  0.035682\n",
              "25  0.000000  0.000000\n",
              "26  0.000000  0.000000\n",
              "27 -0.234406 -0.240482\n",
              "28  0.000000  0.000000\n",
              "29  0.232209  0.225448\n",
              "..       ...       ...\n",
              "70  0.000000  0.000000\n",
              "71  0.130166  0.144485\n",
              "72  0.000000  0.000000\n",
              "73  0.000000  0.000000\n",
              "74  0.000000  0.000000\n",
              "75 -0.178534 -0.186722\n",
              "76  0.000000  0.000000\n",
              "77  0.218493  0.229656\n",
              "78  0.000000  0.000000\n",
              "79  0.000000  0.000000\n",
              "80  0.195579  0.200442\n",
              "81  0.000000  0.000000\n",
              "82  0.000000  0.000000\n",
              "83  0.031153  0.050457\n",
              "84  0.229065  0.231451\n",
              "85 -0.006512 -0.039516\n",
              "86 -0.107947 -0.119896\n",
              "87  0.000000  0.000000\n",
              "88  0.149419  0.171693\n",
              "89  0.000000  0.000000\n",
              "90  0.047955  0.063434\n",
              "91  0.000000  0.003592\n",
              "92 -0.083171 -0.107145\n",
              "93  0.084615  0.101221\n",
              "94 -0.168431 -0.175473\n",
              "95  0.138411  0.152623\n",
              "96  0.000000  0.000000\n",
              "97  0.061161  0.081945\n",
              "98 -0.083348 -0.104929\n",
              "99 -0.141154 -0.153871\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "execution_count": 0,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = tfp.glm.Bernoulli()\n",
        "model_coefficients_start = tf.zeros(x.shape[-1], np.float32)\n",
        "@tf.function(autograph=False)\n",
        "def fit_model():\n",
        "  return tfp.glm.fit_sparse(\n",
        "    model_matrix=tf.convert_to_tensor(x),\n",
        "    response=tf.convert_to_tensor(y),\n",
        "    model=model,\n",
        "    model_coefficients_start=model_coefficients_start,\n",
        "    l1_regularizer=800.,\n",
        "    l2_regularizer=None,\n",
        "    maximum_iterations=10,\n",
        "    maximum_full_sweeps_per_iteration=10,\n",
        "    tolerance=1e-6,\n",
        "    learning_rate=None)\n",
        "\n",
        "model_coefficients, is_converged, num_iter = [t.numpy() for t in fit_model()]\n",
        "coefs_comparison = pd.DataFrame({\n",
        "  'Learned': model_coefficients,\n",
        "  'True': model_coefficients_true,\n",
        "})\n",
        "  \n",
        "print(('is_converged: {}\\n'\n",
        "       '    num_iter: {}\\n\\n'\n",
        "       'Coefficients:').format(\n",
        "    is_converged,\n",
        "    num_iter))\n",
        "coefs_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrJC2J1YbR5L"
      },
      "source": [
        "Tenga en cuenta que los coeficientes aprendidos tienen el mismo patrón de dispersión que los coeficientes verdaderos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ7SzrPZMpke"
      },
      "outputs": [],
      "source": [
        "# Save the learned coefficients to a file.\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_prox.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, model_coefficients, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW9NgB1Zisqh"
      },
      "source": [
        "### Comparación con `glmnet` de R\n",
        "\n",
        "Comparamos la salida del descenso de gradiente proximal por coordenadas con la de `glmnet` de R, que usa un algoritmo similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aptz7SWwkd5v"
      },
      "source": [
        "#### NOTA: Para ejecutar esta sección, se debe cambiar a un tiempo de ejecución de Colab de R."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS1H3n53h9qc"
      },
      "outputs": [],
      "source": [
        "suppressMessages({\n",
        "  library('glmnet')\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X6zKSaxie7I"
      },
      "outputs": [],
      "source": [
        "data_dir <- '/tmp/glm_example'\n",
        "x <- as.matrix(read.csv(paste(data_dir, '/x.csv', sep=''),\n",
        "                        header=FALSE))\n",
        "y <- as.matrix(read.csv(paste(data_dir, '/y.csv', sep=''),\n",
        "                        header=FALSE, colClasses='integer'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb31LbhRjsSz"
      },
      "outputs": [],
      "source": [
        "fit <- glmnet(\n",
        "x = x,\n",
        "y = y,\n",
        "family = \"binomial\",  # Logistic regression\n",
        "alpha = 1,  # corresponds to l1_weight = 1, l2_weight = 0\n",
        "standardize = FALSE,\n",
        "intercept = FALSE,\n",
        "thresh = 1e-30,\n",
        "type.logistic = \"Newton\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTN4RKQbhlCm"
      },
      "outputs": [],
      "source": [
        "write.csv(as.matrix(coef(fit, 0.008)),\n",
        "          paste(data_dir, '/model_coefficients_glmnet.csv', sep=''),\n",
        "          row.names=FALSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsrEKgUGjGjf"
      },
      "source": [
        "#### Compare R, TFP y coeficientes verdaderos (Nota: Vuelva al núcleo de Python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCOlGo_4i2sb"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/tmp/glm_example'\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_glmnet.csv'.format(DATA_DIR),\n",
        "                       'r') as f:\n",
        "  model_coefficients_glmnet = np.loadtxt(f,\n",
        "                                   skiprows=2  # Skip column name and intercept\n",
        "                               )\n",
        "\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_prox.csv'.format(DATA_DIR),\n",
        "                       'r') as f:\n",
        "  model_coefficients_prox = np.loadtxt(f)\n",
        "\n",
        "with tf.io.gfile.GFile(\n",
        "    '{}/model_coefficients_true.csv'.format(DATA_DIR), 'r') as f:\n",
        "  model_coefficients_true = np.loadtxt(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l-SZ85lnKg5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R</th>\n",
              "      <th>TFP</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.281080</td>\n",
              "      <td>0.216240</td>\n",
              "      <td>0.220758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.056625</td>\n",
              "      <td>0.043702</td>\n",
              "      <td>0.063950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.188771</td>\n",
              "      <td>-0.145379</td>\n",
              "      <td>-0.153256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.030112</td>\n",
              "      <td>0.024382</td>\n",
              "      <td>0.046572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.316488</td>\n",
              "      <td>-0.242985</td>\n",
              "      <td>-0.242609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.139214</td>\n",
              "      <td>-0.106168</td>\n",
              "      <td>-0.123367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.050239</td>\n",
              "      <td>-0.039745</td>\n",
              "      <td>-0.067560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.283372</td>\n",
              "      <td>-0.217717</td>\n",
              "      <td>-0.222169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.021815</td>\n",
              "      <td>-0.016553</td>\n",
              "      <td>-0.041692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.024070</td>\n",
              "      <td>0.018959</td>\n",
              "      <td>0.049624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.074039</td>\n",
              "      <td>-0.057686</td>\n",
              "      <td>-0.078299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.005321</td>\n",
              "      <td>0.003642</td>\n",
              "      <td>0.035682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.304958</td>\n",
              "      <td>-0.234406</td>\n",
              "      <td>-0.240482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.301562</td>\n",
              "      <td>0.232209</td>\n",
              "      <td>0.225448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.169291</td>\n",
              "      <td>0.130166</td>\n",
              "      <td>0.144485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>-0.231294</td>\n",
              "      <td>-0.178534</td>\n",
              "      <td>-0.186722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.284215</td>\n",
              "      <td>0.218493</td>\n",
              "      <td>0.229656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.254524</td>\n",
              "      <td>0.195579</td>\n",
              "      <td>0.200442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.040716</td>\n",
              "      <td>0.031153</td>\n",
              "      <td>0.050457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.297475</td>\n",
              "      <td>0.229065</td>\n",
              "      <td>0.231451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>-0.008569</td>\n",
              "      <td>-0.006512</td>\n",
              "      <td>-0.039516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-0.141028</td>\n",
              "      <td>-0.107947</td>\n",
              "      <td>-0.119896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.194130</td>\n",
              "      <td>0.149419</td>\n",
              "      <td>0.171693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.062601</td>\n",
              "      <td>0.047955</td>\n",
              "      <td>0.063434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.107693</td>\n",
              "      <td>-0.083171</td>\n",
              "      <td>-0.107145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.109381</td>\n",
              "      <td>0.084615</td>\n",
              "      <td>0.101221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>-0.218831</td>\n",
              "      <td>-0.168431</td>\n",
              "      <td>-0.175473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.180662</td>\n",
              "      <td>0.138411</td>\n",
              "      <td>0.152623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.078815</td>\n",
              "      <td>0.061161</td>\n",
              "      <td>0.081945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>-0.108332</td>\n",
              "      <td>-0.083348</td>\n",
              "      <td>-0.104929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-0.183284</td>\n",
              "      <td>-0.141154</td>\n",
              "      <td>-0.153871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           R       TFP      True\n",
              "0   0.281080  0.216240  0.220758\n",
              "1   0.000000  0.000000  0.000000\n",
              "2   0.000000  0.000000  0.000000\n",
              "3   0.000000  0.000000  0.000000\n",
              "4   0.000000  0.000000  0.000000\n",
              "5   0.056625  0.043702  0.063950\n",
              "6  -0.188771 -0.145379 -0.153256\n",
              "7   0.000000  0.000000  0.000000\n",
              "8   0.000000  0.000000  0.000000\n",
              "9   0.000000  0.000000  0.000000\n",
              "10  0.000000  0.000000  0.000000\n",
              "11  0.000000  0.000000  0.000000\n",
              "12  0.000000  0.000000  0.000000\n",
              "13  0.030112  0.024382  0.046572\n",
              "14 -0.316488 -0.242985 -0.242609\n",
              "15 -0.139214 -0.106168 -0.123367\n",
              "16  0.000000  0.000000  0.000000\n",
              "17 -0.050239 -0.039745 -0.067560\n",
              "18 -0.283372 -0.217717 -0.222169\n",
              "19  0.000000  0.000000  0.000000\n",
              "20  0.000000  0.000000  0.000000\n",
              "21 -0.021815 -0.016553 -0.041692\n",
              "22  0.024070  0.018959  0.049624\n",
              "23 -0.074039 -0.057686 -0.078299\n",
              "24  0.005321  0.003642  0.035682\n",
              "25  0.000000  0.000000  0.000000\n",
              "26  0.000000  0.000000  0.000000\n",
              "27 -0.304958 -0.234406 -0.240482\n",
              "28  0.000000  0.000000  0.000000\n",
              "29  0.301562  0.232209  0.225448\n",
              "..       ...       ...       ...\n",
              "70  0.000000  0.000000  0.000000\n",
              "71  0.169291  0.130166  0.144485\n",
              "72  0.000000  0.000000  0.000000\n",
              "73  0.000000  0.000000  0.000000\n",
              "74  0.000000  0.000000  0.000000\n",
              "75 -0.231294 -0.178534 -0.186722\n",
              "76  0.000000  0.000000  0.000000\n",
              "77  0.284215  0.218493  0.229656\n",
              "78  0.000000  0.000000  0.000000\n",
              "79  0.000000  0.000000  0.000000\n",
              "80  0.254524  0.195579  0.200442\n",
              "81  0.000000  0.000000  0.000000\n",
              "82  0.000000  0.000000  0.000000\n",
              "83  0.040716  0.031153  0.050457\n",
              "84  0.297475  0.229065  0.231451\n",
              "85 -0.008569 -0.006512 -0.039516\n",
              "86 -0.141028 -0.107947 -0.119896\n",
              "87  0.000000  0.000000  0.000000\n",
              "88  0.194130  0.149419  0.171693\n",
              "89  0.000000  0.000000  0.000000\n",
              "90  0.062601  0.047955  0.063434\n",
              "91  0.000000  0.000000  0.003592\n",
              "92 -0.107693 -0.083171 -0.107145\n",
              "93  0.109381  0.084615  0.101221\n",
              "94 -0.218831 -0.168431 -0.175473\n",
              "95  0.180662  0.138411  0.152623\n",
              "96  0.000000  0.000000  0.000000\n",
              "97  0.078815  0.061161  0.081945\n",
              "98 -0.108332 -0.083348 -0.104929\n",
              "99 -0.183284 -0.141154 -0.153871\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "execution_count": 0,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coefs_comparison = pd.DataFrame({\n",
        "    'TFP': model_coefficients_prox,\n",
        "    'R': model_coefficients_glmnet,\n",
        "    'True': model_coefficients_true,\n",
        "})\n",
        "coefs_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfv0GVXqY74Y"
      },
      "source": [
        "# Detalles del algoritmo para `tfp.glm.fit_sparse`\n",
        "\n",
        "Presentamos el algoritmo como una secuencia de tres modificaciones al método de Newton. En cada uno, la regla de actualización de $\\beta$ se basa en un vector $s$ y una matriz $H$ que se aproximan al gradiente y al hessiano de la probabilidad logarítmica. En el paso $t$, elegimos una coordenada $j^{(t)}$ para cambiar y actualizamos $\\beta$ de acuerdo con la regla de actualización:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  u^{(t)}\n",
        "&:=\n",
        "  \\frac{\n",
        "    \\left(\n",
        "      s^{(t)}\n",
        "    \\right)_{j^{(t)}}\n",
        "  }{\n",
        "    \\left(\n",
        "      H^{(t)}\n",
        "    \\right)_{j^{(t)},\\, j^{(t)}}\n",
        "  }\n",
        "\\\\[3mm]\n",
        "  \\beta^{(t+1)}\n",
        "&:=\n",
        "  \\beta^{(t)}\n",
        "  -\n",
        "  \\alpha\\,\n",
        "  u^{(t)}\n",
        "  \\,\\text{onehot}(j^{(t)})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Esta actualización es un paso similar a Newton con una tasa de aprendizaje $\\alpha$. Excepto por la pieza final (regularización L1), las siguientes modificaciones difieren solo en cómo actualizan $s$ y $H$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH7C1xBWUV7_"
      },
      "source": [
        "## Punto de partida: método de Newton por coordenadas\n",
        "\n",
        "En el método de Newton por coordenadas, establecemos $s$ y $H$ en el gradiente verdadero y el hessiano de la probabilidad logarítmica:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  s^{(t)}_{\\text{vanilla}}\n",
        "&:=\n",
        "  \\left(\n",
        "    \\nabla_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "  \\right)_{\\beta = \\beta^{(t)}}\n",
        "\\\\\n",
        "  H^{(t)}_{\\text{vanilla}}\n",
        "&:=\n",
        "  \\left(\n",
        "    \\nabla^2_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "  \\right)_{\\beta = \\beta^{(t)}}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rJZD6iyUl0v"
      },
      "source": [
        "## Menos evaluaciones del gradiente y del hessiano\n",
        "\n",
        "El gradiente y el hessiano de la probabilidad logarítmica suelen ser costosos de calcular, por lo que a menudo vale la pena aproximarlos. Podemos hacerlo de la siguiente manera:\n",
        "\n",
        "- Por lo general, se aproxima el hessiano como localmente constante y se aproxima el gradiente al primer orden con ayuda del hessiano (aproximado):\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  H_{\\text{approx}}^{(t+1)}\n",
        "&:=\n",
        "  H^{(t)}\n",
        "\\\\\n",
        "  s_{\\text{approx}}^{(t+1)}\n",
        "&:=\n",
        "  s^{(t)}\n",
        "  +\n",
        "  H^{(t)}\n",
        "  \\left(\n",
        "    \\beta^{(t+1)} - \\beta^{(t)}\n",
        "  \\right)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "- De vez en cuando, se ejecuta un paso de actualización \"básico\" como el anterior, configurando $s^{(t+1)}$ con el gradiente exacto y $H^{(t+1)}$ con el hessiano exacto de la probabilidad logarítmica, evaluado en $\\beta^{(t+1)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfvvyaVnUqIQ"
      },
      "source": [
        "## Sustitución de la información negativa de Fisher por el hessiano\n",
        "\n",
        "Para reducir aún más el costo de los pasos de actualización básicos, podemos establecer $H$ en la matriz de información negativa de Fisher (que se puede calcular eficientemente si se usan las fórmulas en \"Ajuste de parámetros del GLM a los datos\" a continuación) en lugar del hessiano exacto:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  H_{\\text{Fisher}}^{(t+1)}\n",
        "&:=\n",
        "  \\mathbb{E}_{Y_i \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta^{(t+1)}), \\phi)}\n",
        "  \\left[\n",
        "    \\left(\n",
        "      \\nabla_\\beta^2\\, \\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{Y})\n",
        "    \\right)_{\\beta = \\beta^{(t+1)}}\n",
        "  \\right]\n",
        "\\\\\n",
        "&=\n",
        "  -\\mathbf{x}^\\top\n",
        "  \\,\\text{diag}\\left(\n",
        "    \\frac{\n",
        "      \\phi\\, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t+1)})^2\n",
        "    }{\n",
        "      {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t+1)})\n",
        "    }\\right)\\,\n",
        "  \\mathbf{x}\n",
        "\\\\\n",
        "  s_{\\text{Fisher}}^{(t+1)}\n",
        "&:=\n",
        "  s_{\\text{vanilla}}^{(t+1)}\n",
        "\\\\\n",
        "&=\n",
        "  \\left(\n",
        "    \\mathbf{x}^\\top\n",
        "    \\,\\text{diag}\\left(\\frac{\n",
        "        {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t+1)})\n",
        "      }{\n",
        "        {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t+1)})\n",
        "      }\\right)\n",
        "    \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta^{(t+1)})\\right)\n",
        "  \\right)\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTH07xYpWGcR"
      },
      "source": [
        "## Regularización L1 mediante descenso de gradiente proximal\n",
        "\n",
        "Para incorporar la regularización L1, reemplazamos la regla de actualización.\n",
        "\n",
        "$$\n",
        "  \\beta^{(t+1)}\n",
        ":=\n",
        "  \\beta^{(t)}\n",
        "  -\n",
        "  \\alpha\\,\n",
        "  u^{(t)}\n",
        "  \\,\\text{onehot}(j^{(t)})\n",
        "$$\n",
        "\n",
        "con la regla de actualización más general\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\gamma^{(t)}\n",
        "&:=\n",
        "  -\\frac{\\alpha\\, r_{\\text{L1}}}{\\left(H^{(t)}\\right)_{j^{(t)},\\, j^{(t)}}}\n",
        "\\\\[2mm]\n",
        "  \\left(\\beta_{\\text{reg}}^{(t+1)}\\right)_j\n",
        "&:=\n",
        "  \\begin{cases}\n",
        "    \\beta^{(t+1)}_j\n",
        "      &\\text{if } j \\neq j^{(t)} \\\\\n",
        "    \\text{SoftThreshold} \\left(\n",
        "      \\beta^{(t)}_j - \\alpha\\, u^{(t)}\n",
        "      ,\\ \n",
        "      \\gamma^{(t)}\n",
        "    \\right)\n",
        "      &\\text{if } j = j^{(t)}\n",
        "  \\end{cases}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "donde $r_{\\text{L1}} &gt; 0$ es una constante proporcionada (el coeficiente de regularización L1) y $\\text{SoftThreshold}$ es el operador de umbral suave, definido por\n",
        "\n",
        "$$\n",
        "\\text{SoftThreshold}(\\beta, \\gamma)\n",
        ":=\n",
        "\\begin{cases}\n",
        "\\beta + \\gamma\n",
        "  &\\text{if } \\beta < -\\gamma\n",
        "\\\\\n",
        "0\n",
        "  &\\text{if } -\\gamma \\leq \\beta \\leq \\gamma\n",
        "\\\\\n",
        "\\beta - \\gamma\n",
        "  &\\text{if } \\beta > \\gamma.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Esta regla de actualización tiene las siguientes dos propiedades inspiradoras, que se explican a continuación:\n",
        "\n",
        "1. En el caso de limitación $r_{\\text{L1}} \\to 0$ (es decir, sin regularización L1), esta regla de actualización es idéntica a la regla de actualización original.\n",
        "\n",
        "2. Esta regla de actualización se puede interpretar como la aplicación de un operador de proximidad cuyo punto fijo es la solución al problema de minimización regularizado L1.\n",
        "\n",
        "$$\n",
        "\\underset{\\beta - \\beta^{(t)} \\in \\text{span}\\{ \\text{onehot}(j^{(t)}) \\}}{\\text{arg min}}\n",
        "\\left(\n",
        "  -\\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "  + r_{\\text{L1}} \\left\\lVert \\beta \\right\\rVert_1\n",
        "\\right).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSs7_osNPLVt"
      },
      "source": [
        "### El caso degenerado $r_{\\text{L1}} = 0$ recupera la regla de actualización original\n",
        "\n",
        "Para ver (1), tenga en cuenta que si $r_{\\text{L1}} = 0$ entonces $\\gamma^{(t)} = 0$, por lo tanto\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\left(\\beta_{\\text{reg}}^{(t+1)}\\right)_{j^{(t)}}\n",
        "&=\n",
        "  \\text{SoftThreshold} \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}} - \\alpha\\, u^{(t)}\n",
        "    ,\\ \n",
        "    0\n",
        "  \\right)\n",
        "\\\\\n",
        "&=\n",
        "  \\beta^{(t)}_{j^{(t)}} - \\alpha\\, u^{(t)}.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Por eso\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\beta_{\\text{reg}}^{(t+1)}\n",
        "&=\n",
        "  \\beta^{(t)} - \\alpha\\, u^{(t)} \\,\\text{onehot}(j^{(t)})\n",
        "\\\\\n",
        "&=\n",
        "  \\beta^{(t+1)}.\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiHy_0NIPT5f"
      },
      "source": [
        "### Operador de proximidad cuyo punto fijo es el MLE regularizado\n",
        "\n",
        "Para ver (2), primero tenga en cuenta (ver [Wikipedia](#3)) que para cualquier $\\gamma > 0$, la siguiente regla de actualización\n",
        "\n",
        "$$\n",
        "  \\left(\\beta_{\\text{exact-prox}, \\gamma}^{(t+1)}\\right)_{j^{(t)}}\n",
        ":=\n",
        "  \\text{prox}_{\\gamma \\lVert \\cdot \\rVert_1}\n",
        "  \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}} + \\frac{\\gamma}{r_{\\text{L1}}}\n",
        "    \\left(\n",
        "      \\left(\n",
        "        \\nabla_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "      \\right)_{\\beta = \\beta^{(t)}}\n",
        "    \\right)_{j^{(t)}}\n",
        "  \\right)\n",
        "$$\n",
        "\n",
        "satisface (2), donde $\\text{prox}$ es el operador de proximidad (ver [Yu](#4), donde este operador se denota $\\mathsf{P}$). El lado derecho de la ecuación anterior se calcula [aquí](#2):\n",
        "\n",
        "$$\n",
        "  \\left(\\beta_{\\text{exact-prox}, \\gamma}^{(t+1)}\\right)_{j^{(t)}}\n",
        "=\n",
        "  \\text{SoftThreshold} \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}}\n",
        "    +\n",
        "    \\frac{\\gamma}{r_{\\text{L1}}}\n",
        "    \\left(\n",
        "      \\left(\n",
        "        \\nabla_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "      \\right)_{\\beta = \\beta^{(t)}}\n",
        "    \\right)_{j^{(t)}}\n",
        "    ,\\ \n",
        "    \\gamma\n",
        "  \\right).\n",
        "$$\n",
        "\n",
        "En particular, al establecer $\\gamma = \\gamma^{(t)} = -\\frac{\\alpha\\, r_{\\text{L1}}}{\\left(H^{(t)}\\right)_{j^{(t)}, j^{(t)}}}$ (tenga en cuenta que $\\gamma^{(t)} > 0$ siempre que la probabilidad logarítmica negativa sea convexa), obtenemos la regla de actualización\n",
        "\n",
        "$$\n",
        "  \\left(\\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)}\\right)_{j^{(t)}}\n",
        "=\n",
        "  \\text{SoftThreshold} \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}}\n",
        "    -\n",
        "    \\alpha\n",
        "    \\frac{\n",
        "      \\left(\n",
        "        \\left(\n",
        "          \\nabla_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "        \\right)_{\\beta = \\beta^{(t)}}\n",
        "      \\right)_{j^{(t)}}\n",
        "    }{\n",
        "      \\left(H^{(t)}\\right)_{j^{(t)}, j^{(t)}}\n",
        "    }\n",
        "    ,\\ \n",
        "    \\gamma^{(t)}\n",
        "  \\right).\n",
        "$$\n",
        "\n",
        "Luego reemplazamos el gradiente exacto $\\left(\n",
        "\\nabla_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "\\right)_{\\beta = \\beta^{(t)}}$ con su aproximación $s^{(t)}$, y obtenemos\n",
        "\n",
        "\\begin{align*}\n",
        "  \\left(\\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)}\\right)_{j^{(t)}}\n",
        "&\\approx\n",
        "  \\text{SoftThreshold} \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}}\n",
        "    -\n",
        "    \\alpha\n",
        "    \\frac{\n",
        "      \\left(s^{(t)}\\right)_{j^{(t)}}\n",
        "    }{\n",
        "      \\left(H^{(t)}\\right)_{j^{(t)}, j^{(t)}}\n",
        "    }\n",
        "    ,\\ \n",
        "    \\gamma^{(t)}\n",
        "  \\right)\n",
        "\\\\\n",
        "&=\n",
        "    \\text{SoftThreshold} \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}}\n",
        "    -\n",
        "    \\alpha\\,\n",
        "    u^{(t)}\n",
        "    ,\\ \n",
        "    \\gamma^{(t)}\n",
        "  \\right).\n",
        "\\end{align*}\n",
        "\n",
        "Por eso\n",
        "\n",
        "$$\n",
        "  \\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)}\n",
        "\\approx\n",
        "  \\beta_{\\text{reg}}^{(t+1)}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7YOOrmI8j0L"
      },
      "source": [
        "# Derivación de hechos del GLM\n",
        "\n",
        "En esta sección, declaramos con todo detalle y derivamos los resultados sobre los GLM que se utilizan en las secciones anteriores. Luego, utilizamos `gradients` de TensorFlow para verificar numéricamente las fórmulas derivadas para el gradiente de la probabilidad logarítmica y la información de Fisher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkHZyhuAIW-p"
      },
      "source": [
        "## Puntuación e información de Fisher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbyYy0bE8pOK"
      },
      "source": [
        "Considere una familia de distribuciones de probabilidad parametrizadas por el vector de parámetros $\\theta$, que tiene densidades de probabilidad $\\left\\{p(\\cdot | \\theta)\\right\\}_{\\theta \\in \\mathcal{T}}$. La **puntuación** de un resultado $y$ en el vector de parámetros $\\theta_0$ se define como el gradiente de la probabilidad logarítmica de $y$ (evaluada en $\\theta_0$), es decir,\n",
        "\n",
        "$$\n",
        "\\text{score}(y, \\theta_0) := \\left[\\nabla_\\theta\\, \\log p(y | \\theta)\\right]_{\\theta=\\theta_0}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYGaMPIx8uOc"
      },
      "source": [
        "### Afirmación: La expectativa de la puntuación es cero\n",
        "\n",
        "En condiciones de regularidad leves (que nos permiten pasar la diferenciación bajo la integral),\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\text{score}(Y, \\theta_0)\\right] = 0.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3H-wNmJ800R"
      },
      "source": [
        "#### Prueba\n",
        "\n",
        "Tenemos\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\text{score}(Y, \\theta_0)\\right]\n",
        "&:=\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\left(\\nabla_\\theta \\log p(Y|\\theta)\\right)_{\\theta=\\theta_0}\\right] \\\\\n",
        "&\\stackrel{\\text{(1)}}{=} \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\frac{\\left(\\nabla_\\theta p(Y|\\theta)\\right)_{\\theta=\\theta_0}}{p(Y|\\theta=\\theta_0)}\\right] \\\\\n",
        "&\\stackrel{\\text{(2)}}{=} \\int_{\\mathcal{Y}} \\left[\\frac{\\left(\\nabla_\\theta p(y|\\theta)\\right)_{\\theta=\\theta_0}}{p(y|\\theta=\\theta_0)}\\right] p(y | \\theta=\\theta_0)\\, dy \\\\\n",
        "&= \\int_{\\mathcal{Y}} \\left(\\nabla_\\theta p(y|\\theta)\\right)_{\\theta=\\theta_0}\\, dy \\\\\n",
        "&\\stackrel{\\text{(3)}}{=} \\left[\\nabla_\\theta \\left(\\int_{\\mathcal{Y}} p(y|\\theta)\\, dy\\right) \\right]_{\\theta=\\theta_0} \\\\\n",
        "&\\stackrel{\\text{(4)}}{=} \\left[\\nabla_\\theta\\, 1 \\right]_{\\theta=\\theta_0} \\\\\n",
        "&= 0,\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "donde hemos utilizado: (1) regla de la cadena para la diferenciación, (2) definición de expectativa, (3) paso de la diferenciación bajo el signo integral (mediante el uso de las condiciones de regularidad), (4) la integral de una densidad de probabilidad es 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y1DPVOI9OT2"
      },
      "source": [
        "### Afirmación (información de Fisher): La varianza de la puntuación es igual a la probabilidad hessiana negativa esperada de la probabilidad logarítmica.\n",
        "\n",
        "En condiciones de regularidad leves (que nos permiten pasar la diferenciación bajo la integral),\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "\\text{score}(Y, \\theta_0) \\text{score}(Y, \\theta_0)^\\top\n",
        "\\right]\n",
        "=\n",
        "-\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "\\left(\\nabla_\\theta^2 \\log p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "donde $\\nabla_\\theta^2 F$ denota la matriz hessiana, cuya entrada $(i, j)$ es $\\frac{\\partial^2 F}{\\partial \\theta_i \\partial \\theta_j}$.\n",
        "\n",
        "El lado izquierdo de esta ecuación se llama **información de Fisher** de la familia $\\left\\{p(\\cdot | \\theta)\\right\\}_{\\theta \\in \\mathcal{T}}$ en el vector de parámetros $\\theta_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF-ac0Bk-HmR"
      },
      "source": [
        "#### Prueba de la afirmación\n",
        "\n",
        "Tenemos\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "\\left(\\nabla_\\theta^2 \\log p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "\\right]\n",
        "&\\stackrel{\\text{(1)}}{=} \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "  \\left(\\nabla_\\theta^\\top \\frac{\n",
        "    \\nabla_\\theta p(Y | \\theta)\n",
        "  }{\n",
        "    p(Y|\\theta)\n",
        "  }\\right)_{\\theta=\\theta_0}\n",
        "\\right] \\\\\n",
        "&\\stackrel{\\text{(2)}}{=} \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "  \\frac{\n",
        "    \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\n",
        "  -\n",
        "  \\left(\\frac{\n",
        "    \\left(\\nabla_\\theta\\, p(Y|\\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\\right)\n",
        "  \\left(\\frac{\n",
        "    \\left(\\nabla_\\theta\\, p(Y|\\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\\right)^\\top\n",
        "\\right] \\\\\n",
        "&\\stackrel{\\text{(3)}}{=} \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "  \\frac{\n",
        "    \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\n",
        "  -\n",
        "  \\text{score}(Y, \\theta_0)\n",
        "  \\,\\text{score}(Y, \\theta_0)^\\top\n",
        "\\right],\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "donde hemos usado (1) la regla de la cadena para la diferenciación, (2) la regla del cociente para la diferenciación, (3) la regla de la cadena nuevamente, a la inversa.\n",
        "\n",
        "Para completar la demostración, basta demostrar lo siguiente:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "  \\frac{\n",
        "    \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\n",
        "\\right]\n",
        "\\stackrel{\\text{?}}{=}\n",
        "0.\n",
        "$$\n",
        "\n",
        "Para hacer eso, pasamos la diferenciación bajo el signo integral dos veces:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "  \\frac{\n",
        "    \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\n",
        "\\right]\n",
        "&= \\int_{\\mathcal{Y}}\n",
        "  \\left[\n",
        "  \\frac{\n",
        "    \\left(\\nabla^2_\\theta p(y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(y|\\theta=\\theta_0)\n",
        "  }\n",
        "  \\right]\n",
        "  \\, p(y | \\theta=\\theta_0)\\, dy \\\\\n",
        "&= \\int_{\\mathcal{Y}}\n",
        "  \\left(\\nabla^2_\\theta p(y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  \\, dy \\\\\n",
        "&= \\left[\n",
        "    \\nabla_\\theta^2 \\left(\n",
        "      \\int_{\\mathcal{Y}} p(y | \\theta) \\, dy\n",
        "    \\right)\n",
        "  \\right]_{\\theta=\\theta_0} \\\\\n",
        "&= \\left[\n",
        "    \\nabla_\\theta^2 \\, 1\n",
        "  \\right]_{\\theta=\\theta_0} \\\\\n",
        "&= 0.\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAIJfX7IX_lP"
      },
      "source": [
        "### Lema sobre la derivada de la función de partición logarítmica\n",
        "\n",
        "Si $a$, $b$ y $c$ son funciones con valores escalares, $c$ dos veces diferenciables, de modo que la familia de distribuciones $\\left\\{p(\\cdot | \\theta)\\right\\}_{\\theta \\in \\mathcal{T}}$ definido por\n",
        "\n",
        "$$\n",
        "p(y|\\theta) = a(y) \\exp\\left(b(y)\\, \\theta - c(\\theta)\\right)\n",
        "$$\n",
        "\n",
        "cumple las condiciones de regularidad leves que permiten pasar la diferenciación con respecto a $\\theta$ bajo una integral con respecto a $y$, entonces\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right]\n",
        "= c'(\\theta_0)\n",
        "$$\n",
        "\n",
        "y\n",
        "\n",
        "$$\n",
        "\\text{Var}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right]\n",
        "= c''(\\theta_0).\n",
        "$$\n",
        "\n",
        "(Aquí $'$ denota diferenciación, por lo que $c'$ y $c''$ son la primera y la segunda derivada de $c$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYBH-KwpfWhr"
      },
      "source": [
        "#### Prueba\n",
        "\n",
        "Para esta familia de distribuciones, tenemos $\\text{score}(y, \\theta_0) = b(y) - c'(\\theta_0)$. La primera ecuación se deriva entonces del hecho $\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\text{score}(y, \\theta_0) \\right] = 0$. A continuación, tenemos\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{Var}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right]\n",
        "&= \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\left(b(Y) - c'(\\theta_0)\\right)^2 \\right] \\\\\n",
        "&= \\text{the one entry of } \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\text{score}(y, \\theta_0) \\text{score}(y, \\theta_0)^\\top \\right] \\\\\n",
        "&= \\text{the one entry of } -\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\left(\\nabla_\\theta^2 \\log p(\\cdot | \\theta)\\right)_{\\theta=\\theta_0} \\right] \\\\\n",
        "&= -\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ -c''(\\theta_0) \\right] \\\\\n",
        "&= c''(\\theta_0).\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYpWUvvKcX-e"
      },
      "source": [
        "## Familia exponencial de distribuciones sobredispersas\n",
        "\n",
        "Una **familia exponencial (escalar) de distribuciones sobredispersas** es una familia de distribuciones cuyas densidades toman la forma\n",
        "\n",
        "$$\n",
        "p_{\\text{OEF}(m,  T)}(y\\, |\\, \\theta, \\phi) = m(y, \\phi) \\exp\\left(\\frac{\\theta\\, T(y) - A(\\theta)}{\\phi}\\right),\n",
        "$$\n",
        "\n",
        "donde $m$ y $T$ son funciones escalares conocidas, y $\\theta$ y $\\phi$ son parámetros escalares.\n",
        "\n",
        "*[Tenga en cuenta que $A$ está sobredeterminado: para cualquier $\\phi_0$, la función $A$ está completamente determinada por la restricción de que $\\int p_{\\text{OEF}(m, T)}(y\\ |\\ \\theta, \\phi=\\phi_0)\\, dy = 1$ para todos los $\\theta$. Los $A$ producidos por diferentes valores de $\\phi_0$ deben ser todos iguales, lo que impone una restricción a las funciones $m$ y $T$].*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgpoijwPf7TV"
      },
      "source": [
        "### Media y varianza del estadístico suficiente\n",
        "\n",
        "En las mismas condiciones que en el \"Lema sobre la derivada de la función de partición logarítmica\", tenemos\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[\n",
        "T(Y)\n",
        "\\right]\n",
        "=\n",
        "A'(\\theta)\n",
        "$$\n",
        "\n",
        "y\n",
        "\n",
        "$$\n",
        "\\text{Var}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[\n",
        "T(Y)\n",
        "\\right]\n",
        "=\n",
        "\\phi A''(\\theta).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyf51flphGOK"
      },
      "source": [
        "#### Prueba\n",
        "\n",
        "De acuerdo con el \"Lema sobre la derivada de la función de partición logarítmica\", tenemos\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[\n",
        "\\frac{T(Y)}{\\phi}\n",
        "\\right]\n",
        "=\n",
        "\\frac{A'(\\theta)}{\\phi}\n",
        "$$\n",
        "\n",
        "y\n",
        "\n",
        "$$\n",
        "\\text{Var}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[\n",
        "\\frac{T(Y)}{\\phi}\n",
        "\\right]\n",
        "=\n",
        "\\frac{A''(\\theta)}{\\phi}.\n",
        "$$\n",
        "\n",
        "El resultado se deriva entonces del hecho de que la expectativa es lineal ($\\mathbb{E}[aX] = a\\mathbb{E}[X]$) y la varianza es homogénea de grado 2 ($\\text{Var}[aX] = a^2 \\,\\text{Var}[X]$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYOnAZv9d4XH"
      },
      "source": [
        "## Modelo lineal generalizado\n",
        "\n",
        "En un modelo lineal generalizado, una distribución predictiva para la variable de respuesta $Y$ se asocia con un vector de predictores observados $x$. La distribución es miembro de una familia exponencial de distribuciones sobredispersas y el parámetro $\\theta$ se reemplaza por $h(\\eta)$ donde $h$ es una función conocida, $\\eta := x^\\top \\beta$ es la llamada **respuesta lineal** y $\\beta$ es un vector de parámetros (coeficientes de regresión) que se deben aprender. En general, el parámetro de dispersión $\\phi$ también se puede aprender, pero en nuestra configuración trataremos $\\phi$ como si fuera conocido. Entonces nuestra configuración será la siguiente:\n",
        "\n",
        "$$\n",
        "Y \\sim p_{\\text{OEF}(m, T)}(\\cdot\\, |\\, \\theta = h(\\eta), \\phi)\n",
        "$$\n",
        "\n",
        "donde la estructura del modelo se caracteriza por la distribución $p_{\\text{OEF}(m, T)}$ y la función $h$ que convierte la respuesta lineal en parámetros.\n",
        "\n",
        "Tradicionalmente, la asignación de la respuesta lineal $\\eta$ al significado $\\mu := \\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot\\, |\\, \\theta = h(\\eta), \\phi)}\\left[ Y\\right]$ se denota así:\n",
        "\n",
        "$$\n",
        "\\mu = g^{-1}(\\eta).\n",
        "$$\n",
        "\n",
        "Se requiere que esta asignación sea uno a uno, y su inverso, $g$, se denomina **función de enlace** para este GLM. Normalmente, se describe un GLM nombrando su función de enlace y su familia de distribuciones; por ejemplo, un \"GLM con distribución de Bernoulli y función de enlace logit\" (también conocido como modelo de regresión logística). Para caracterizar completamente el GLM, también se debe especificar la función $h$. Si $h$ es la identidad, entonces se dice que $g$ es la **función de enlace canónico**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-mrWHH2-wtv"
      },
      "source": [
        "### Afirmación: $h'$ se expresa en términos del estadístico suficiente\n",
        "\n",
        "Defina\n",
        "\n",
        "$$\n",
        "{\\text{Mean}_T}(\\eta)\n",
        ":=\n",
        "\\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(\\eta), \\phi)} \\left[\n",
        "  T(Y)\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "y\n",
        "\n",
        "$$\n",
        "{\\text{Var}_T}(\\eta)\n",
        ":=\n",
        "\\text{Var}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(\\eta), \\phi)} \\left[\n",
        "  T(Y)\n",
        "\\right].\n",
        "$$\n",
        "\n",
        "Entonces, tenemos\n",
        "\n",
        "$$\n",
        "h'(\\eta) = \\frac{\\phi\\, {\\text{Mean}_T}'(\\eta)}{{\\text{Var}_T}(\\eta)}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z36iGKlf_-3F"
      },
      "source": [
        "#### Prueba\n",
        "\n",
        "Mediante la \"Media y varianza del estadístico suficiente\", tenemos\n",
        "\n",
        "$$\n",
        "{\\text{Mean}_T}(\\eta) = A'(h(\\eta)).\n",
        "$$\n",
        "\n",
        "Al derivar con la regla de la cadena obtenemos $$\n",
        "{\\text{Mean}_T}'(\\eta) = A''(h(\\eta))\\, h'(\\eta),\n",
        "$$\n",
        "\n",
        "y mediante la \"Media y varianza del estadístico suficiente\",\n",
        "\n",
        "$$\n",
        "\\cdots = \\frac{1}{\\phi} {\\text{Var}_T}(\\eta)\\ h'(\\eta).\n",
        "$$\n",
        "\n",
        "La conclusión es la siguiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8LV_QHPx-wV"
      },
      "source": [
        "## Ajuste de parámetros del GLM a los datos\n",
        "\n",
        "Las propiedades derivadas anteriormente se prestan muy bien para ajustar los parámetros del GLM $\\beta$ a un conjunto de datos. Los métodos cuasi-Newton, como la puntuación de Fisher, se basan en el gradiente del logaritmo de probabilidad y la información de Fisher, que ahora mostramos se puede calcular de manera especialmente eficiente para un GLM.\n",
        "\n",
        "Supongamos que hemos observado vectores predictores $x_i$ y respuestas escalares asociadas $y_i$. En forma matricial, diremos que hemos observado predictores $\\mathbf{x}$ y respuesta $\\mathbf{y}$, donde $\\mathbf{x}$ es la matriz cuya $i$ésima fila es $x_i^ \\top$ y $\\mathbf{y}$ es el vector cuyo $i$ésimo elemento es $y_i$. La probabilidad logarítmica de los parámetros $\\beta$ es entonces la siguiente:\n",
        "\n",
        "$$\n",
        "\\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{N} \\log p_{\\text{OEF}(m, T)}(y_i\\, |\\, \\theta = h(x_i^\\top \\beta), \\phi).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aghNxiO_HFW1"
      },
      "source": [
        "### Para una sola muestra de datos\n",
        "\n",
        "Para simplificar la notación, consideremos primero el caso de un único punto de datos, $N=1$; luego, extenderemos al caso general por aditividad.\n",
        "\n",
        "#### Gradiente\n",
        "\n",
        "Tenemos\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\ell(\\beta\\, ;\\, x, y)\n",
        "&= \\log p_{\\text{OEF}(m, T)}(y\\, |\\, \\theta = h(x^\\top \\beta), \\phi) \\\\\n",
        "&= \\log m(y, \\phi) + \\frac{\\theta\\, T(y) - A(\\theta)}{\\phi}, \\quad\\text{where}\\  \\theta = h(x^\\top \\beta).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Por lo tanto, según la regla de la cadena,\n",
        "\n",
        "$$\n",
        "\\nabla_\\beta \\ell(\\beta\\, ; \\, x, y) = \\frac{T(y) - A'(\\theta)}{\\phi}\\, h'(x^\\top \\beta)\\, x.\n",
        "$$\n",
        "\n",
        "Por separado, mediante la \"Media y varianza del estadístico suficiente\", tenemos $A'(\\theta) = {\\text{Media}_T}(x^\\top \\beta)$. Por lo tanto, según \"Afirmación: $h'$ ser expresa en términos del estadístico suficiente\", tenemos\n",
        "\n",
        "$$\n",
        "\\cdots =\n",
        "  \\left(T(y) - {\\text{Mean}_T}(x^\\top \\beta)\\right)\n",
        "  \\frac{{\\text{Mean}_T}'(x^\\top \\beta)}{{\\text{Var}_T}(x^\\top \\beta)}\n",
        "  \\,x.\n",
        "$$\n",
        "\n",
        "#### Hessiano\n",
        "\n",
        "Al derivar por segunda vez, de acuerdo con la regla del producto obtenemos\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla_\\beta^2 \\ell(\\beta\\, ;\\, x, y)\n",
        "&=\n",
        "  \\left[\n",
        "    -A''(h(x^\\top \\beta))\\, h'(x^\\top \\beta)\n",
        "  \\right]\n",
        "  h'(x^\\top \\beta)\\, x x^\\top\n",
        "  +\n",
        "  \\left[\n",
        "    T(y) - A'(h(x^\\top \\beta))\n",
        "  \\right]\n",
        "  h''(x^\\top \\beta)\\, xx^\\top\n",
        "  ] \\\\\n",
        "&=\n",
        "  \\left(\n",
        "    -{\\text{Mean}_T}'(x^\\top \\beta)\\, h'(x^\\top \\beta)\n",
        "    +\n",
        "    \\left[T(y) - A'(h(x^\\top \\beta))\\right]\n",
        "  \\right)\\, x x^\\top.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "#### Información de Fisher\n",
        "\n",
        "Mediante la \"Media y varianza del estadístico suficiente\", tenemos\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x^\\top \\beta), \\phi)} \\left[\n",
        "T(y) - A'(h(x^\\top \\beta))\n",
        "\\right] = 0.\n",
        "$$\n",
        "\n",
        "Por eso\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x^\\top \\beta), \\phi)} \\left[\n",
        "  \\nabla_\\beta^2 \\ell(\\beta\\, ;\\, x, y)\n",
        "\\right]\n",
        "&=\n",
        "  -{\\text{Mean}_T}'(x^\\top \\beta)\\, h'(x^\\top \\beta) x x^\\top \\\\\n",
        "&=\n",
        "  -\\frac{\\phi\\, {\\text{Mean}_T}'(x^\\top \\beta)^2}{{\\text{Var}_T}(x^\\top \\beta)}\\, x x^\\top.\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrA1A583HOng"
      },
      "source": [
        "### Para múltiples muestras de datos\n",
        "\n",
        "Ahora extendemos el caso $N=1$ al caso general. Digamos que $\\boldsymbol{\\eta} := \\mathbf{x} \\beta$ denota el vector cuya $i$ésima coordenada es la respuesta lineal de la $i$ésima muestra de datos. Supongamos que $\\mathbf{T}$ (resp. ${\\textbf{Mean}_T}$, resp. ${\\textbf{Var}_T}$) denota la función transmitida (vectorizada) que aplica la función de valor escalar $ T$ (resp. ${\\text{Mean}_T}$, resp. ${\\text{Var}_T}$) a cada coordenada. Entonces tenemos\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla_\\beta \\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{y})\n",
        "&= \\sum_{i=1}^{N} \\nabla_\\beta \\ell(\\beta\\, ;\\, x_i, y_i) \\\\\n",
        "&= \\sum_{i=1}^{N}\n",
        "  \\left(T(y) - {\\text{Mean}_T}(x_i^\\top \\beta)\\right)\n",
        "  \\frac{{\\text{Mean}_T}'(x_i^\\top \\beta)}{{\\text{Var}_T}(x_i^\\top \\beta)}\n",
        "  \\, x_i \\\\\n",
        "&=\n",
        "  \\mathbf{x}^\\top\n",
        "  \\,\\text{diag}\\left(\\frac{\n",
        "      {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)\n",
        "    }{\n",
        "      {\\textbf{Var}_T}(\\mathbf{x} \\beta)\n",
        "    }\\right)\n",
        "  \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta)\\right) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "y\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{Y_i \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta), \\phi)} \\left[\n",
        "  \\nabla_\\beta^2 \\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{Y})\n",
        "\\right]\n",
        "&= \\sum_{i=1}^{N} \\mathbb{E}_{Y_i \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta), \\phi)} \\left[\n",
        "  \\nabla_\\beta^2 \\ell(\\beta\\, ;\\, x_i, Y_i)\n",
        "\\right] \\\\\n",
        "&= \\sum_{i=1}^{N}\n",
        "  -\\frac{\\phi\\, {\\text{Mean}_T}'(x_i^\\top \\beta)^2}{{\\text{Var}_T}(x_i^\\top \\beta)}\\, x_i x_i^\\top \\\\\n",
        "&=\n",
        "  -\\mathbf{x}^\\top\n",
        "  \\,\\text{diag}\\left(\n",
        "    \\frac{\n",
        "      \\phi\\, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)^2\n",
        "    }{\n",
        "      {\\textbf{Var}_T}(\\mathbf{x} \\beta)\n",
        "    }\\right)\\,\n",
        "  \\mathbf{x},\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "donde las fracciones denotan división por elementos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUrOmdt395hZ"
      },
      "source": [
        "## Cómo verificar las fórmulas numéricamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVp59IBW-TK6"
      },
      "source": [
        "Ahora usamos `tf.gradients` para verificar numéricamente la fórmula anterior para el gradiente de la probabilidad logarítmica y verificamos la fórmula para la información de Fisher con una estimación de Monte Carlo con ayuda de `tf.hessians`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM-HDPdPepE-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coordinatewise relative error between naively computed gradients and formula-based gradients (should be zero):\n",
            "[[2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]]\n",
            "\n",
            "Coordinatewise relative error between average of naively computed Hessian and formula-based FIM (should approach zero as num_trials -> infinity):\n",
            "[[0.00072369 0.00072369 0.00072369]\n",
            " [0.00072369 0.00072369 0.00072369]\n",
            " [0.00072369 0.00072369 0.00072369]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def VerifyGradientAndFIM():\n",
        "  model = tfp.glm.BernoulliNormalCDF()\n",
        "  model_matrix = np.array([[1., 5, -2],\n",
        "                           [8, -1, 8]])\n",
        "\n",
        "  def _naive_grad_and_hessian_loss_fn(x, response):\n",
        "    # Computes gradient and Hessian of negative log likelihood using autodiff.\n",
        "    predicted_linear_response = tf.linalg.matvec(model_matrix, x)\n",
        "    log_probs = model.log_prob(response, predicted_linear_response)\n",
        "    grad_loss = tf.gradients(-log_probs, [x])[0]\n",
        "    hessian_loss = tf.hessians(-log_probs, [x])[0]\n",
        "    return [grad_loss, hessian_loss]\n",
        "\n",
        "  def _grad_neg_log_likelihood_and_fim_fn(x, response):\n",
        "    # Computes gradient of negative log likelihood and Fisher information matrix\n",
        "    # using the formulas above.\n",
        "    predicted_linear_response = tf.linalg.matvec(model_matrix, x)\n",
        "    mean, variance, grad_mean = model(predicted_linear_response)\n",
        "\n",
        "    v = (response - mean) * grad_mean / variance\n",
        "    grad_log_likelihood = tf.linalg.matvec(model_matrix, v, adjoint_a=True)\n",
        "    w = grad_mean**2 / variance\n",
        "\n",
        "    fisher_info = tf.linalg.matmul(\n",
        "        model_matrix,\n",
        "        w[..., tf.newaxis] * model_matrix,\n",
        "        adjoint_a=True)\n",
        "    return [-grad_log_likelihood, fisher_info]\n",
        "\n",
        "  @tf.function(autograph=False)\n",
        "  def compute_grad_hessian_estimates():\n",
        "    # Monte Carlo estimate of E[Hessian(-LogLikelihood)], where the expectation is\n",
        "    # as written in \"Claim (Fisher information)\" above.\n",
        "    num_trials = 20\n",
        "    trial_outputs = []\n",
        "    np.random.seed(10)\n",
        "    model_coefficients_ = np.random.random(size=(model_matrix.shape[1],))\n",
        "    model_coefficients = tf.convert_to_tensor(model_coefficients_)\n",
        "    for _ in range(num_trials):\n",
        "      # Sample from the distribution of `model`\n",
        "      response = np.random.binomial(\n",
        "          1,\n",
        "          scipy.stats.norm().cdf(np.matmul(model_matrix, model_coefficients_))\n",
        "      ).astype(np.float64)\n",
        "      trial_outputs.append(\n",
        "          list(_naive_grad_and_hessian_loss_fn(model_coefficients, response)) +\n",
        "          list(\n",
        "              _grad_neg_log_likelihood_and_fim_fn(model_coefficients, response))\n",
        "      )\n",
        "\n",
        "    naive_grads = tf.stack(\n",
        "        list(naive_grad for [naive_grad, _, _, _] in trial_outputs), axis=0)\n",
        "    fancy_grads = tf.stack(\n",
        "        list(fancy_grad for [_, _, fancy_grad, _] in trial_outputs), axis=0)\n",
        "\n",
        "    average_hess = tf.reduce_mean(tf.stack(\n",
        "        list(hess for [_, hess, _, _] in trial_outputs), axis=0), axis=0)\n",
        "    [_, _, _, fisher_info] = trial_outputs[0]\n",
        "    return naive_grads, fancy_grads, average_hess, fisher_info\n",
        "  \n",
        "  naive_grads, fancy_grads, average_hess, fisher_info = [\n",
        "      t.numpy() for t in compute_grad_hessian_estimates()]\n",
        "\n",
        "  print(\"Coordinatewise relative error between naively computed gradients and\"\n",
        "        \" formula-based gradients (should be zero):\\n{}\\n\".format(\n",
        "            (naive_grads - fancy_grads) / naive_grads))\n",
        "\n",
        "  print(\"Coordinatewise relative error between average of naively computed\"\n",
        "        \" Hessian and formula-based FIM (should approach zero as num_trials\"\n",
        "        \" -> infinity):\\n{}\\n\".format(\n",
        "                (average_hess - fisher_info) / average_hess))\n",
        "    \n",
        "VerifyGradientAndFIM()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAiNubQ-WDHN"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "<a name=\"1\"></a>[1]: Guo-Xun Yuan, Chia-Hua Ho y Chih-Jen Lin. An Improved GLMNET for L1-regularized Logistic Regression. *Journal of Machine Learning Research*, 13, 2012. http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf\n",
        "\n",
        "<a name=\"2\"></a>[2]: skd. Derivation of Soft Thresholding Operator.  2018. https://math.stackexchange.com/q/511106\n",
        "\n",
        "<a name=\"3\"></a>[3]: Contribuyentes de Wikipedia. Proximal gradient methods for learning. *Wikipedia, The Free Encyclopedia*, 2018. https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning\n",
        "\n",
        "<a name=\"4\"></a>[4]: Yao-Liang Yu. The Proximity Operator. https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Generalized_Linear_Models.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
