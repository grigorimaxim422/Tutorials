{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckM5wJMsNTYL"
      },
      "source": [
        "##### Copyright 2023 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NKvERjPVNWxu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqePLdDjNhNk"
      },
      "source": [
        "# JAX2TF를 사용하여 JAX 모델 가져오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw3w46yhNiK_"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/jax2tf\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/guide/jax2tf.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/guide/jax2tf.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/guide/jax2tf.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\"><br>노트북 다운로드하기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyrsY3uTOmPY"
      },
      "source": [
        "이 노트북은 [JAX](https://jax.readthedocs.io/en/latest/)를 사용하는 모델을 생성하고 이를 TensorFlow로 가져와 훈련을 계속하는 완전하고 실행 가능한 예제를 제공합니다. 이 작업은 JAX 생태계에서 TensorFlow 생태계로 이동하는 길을 제공하는 경량 API인 [JAX2TF](https://github.com/google/jax/tree/main/jax/experimental/jax2tf)를 통해 가능합니다.\n",
        "\n",
        "JAX는 고성능 배열 컴퓨팅 라이브러리입니다. 모델을 생성하기 위해 이 노트북은 JAX용 신경망 라이브러리인 [Flax](https://flax.readthedocs.io/en/latest/)를 사용합니다. 모델을 훈련하기 위해 JAX용 최적화 라이브러리인 [Optax](https://optax.readthedocs.io)를 사용합니다.\n",
        "\n",
        "JAX를 사용하는 연구자에게 JAX2TF는 TensorFlow의 검증된 도구를 사용하여 프로덕션으로 이동하는 길을 제공합니다.\n",
        "\n",
        "이 기능을 유용하게 사용할 수 있는 방법은 여러 가지가 있지만 그 중 몇 가지만 소개하겠습니다.\n",
        "\n",
        "- 추론: JAX용으로 작성된 모델을 가져와 TF Serving을 사용하는 서버에 배포하거나, TFLite를 사용하는 온디바이스(on-device)에 배포하거나, TensorFlow.js를 사용하는 웹에 배포할 수 있습니다.\n",
        "\n",
        "- 미세 조정: JAX를 사용하여 훈련한 모델의 구성 요소를 JAX2TF를 사용하는 TF로 가져온 다음, 기존 훈련 데이터와 설정을 사용하는 TensorFlow에서 계속 훈련할 수 있습니다.\n",
        "\n",
        "- 융합: 유연성을 극대화하기 위해 JAX를 사용하여 훈련한 모델의 일부와 TensorFlow를 사용하여 훈련한 모델의 일부를 결합합니다.\n",
        "\n",
        "JAX와 TensorFlow 사이의 이러한 상호 운용을 가능하게 하는 핵심은 `jax2tf.convert`이며 이 기능은 JAX에서 생성된 모델 구성 요소(손실 함수, 예측 함수 등)를 가져와서 이를 TensorFlow 함수와 동등한 모습을 갖도록 만든 다음 TensorFlow SavedModel로 내보낼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6rtu96yOepm"
      },
      "source": [
        "## 설치하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yqxfHzr0LPF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "import optax\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from jax.experimental import jax2tf\n",
        "from threading import Lock # Only used in the visualization utility.\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDnTaZO0r872"
      },
      "outputs": [],
      "source": [
        "# Needed for TensorFlow and JAX to coexist in GPU memory.\n",
        "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized.\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BXOjCNJxDLil"
      },
      "outputs": [],
      "source": [
        "#@title Visualization utilities\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
        "\n",
        "# The utility for displaying training and validation curves.\n",
        "def display_train_curves(loss, avg_loss, eval_loss, eval_accuracy, epochs, steps_per_epochs, ignore_first_n=10):\n",
        "\n",
        "  ignore_first_n_epochs = int(ignore_first_n/steps_per_epochs)\n",
        "\n",
        "  # The losses.\n",
        "  ax = plt.subplot(121)\n",
        "  if loss is not None:\n",
        "    x = np.arange(len(loss)) / steps_per_epochs #* epochs\n",
        "    ax.plot(x, loss)\n",
        "  ax.plot(range(1, epochs+1), avg_loss, \"-o\", linewidth=3)\n",
        "  ax.plot(range(1, epochs+1), eval_loss, \"-o\", linewidth=3)\n",
        "  ax.set_title('Loss')\n",
        "  ax.set_ylabel('loss')\n",
        "  ax.set_xlabel('epoch')\n",
        "  if loss is not None:\n",
        "    ax.set_ylim(0, np.max(loss[ignore_first_n:]))\n",
        "    ax.legend(['train', 'avg train', 'eval'])\n",
        "  else:\n",
        "    ymin = np.min(avg_loss[ignore_first_n_epochs:])\n",
        "    ymax = np.max(avg_loss[ignore_first_n_epochs:])\n",
        "    ax.set_ylim(ymin-(ymax-ymin)/10, ymax+(ymax-ymin)/10)\n",
        "    ax.legend(['avg train', 'eval'])\n",
        "\n",
        "  # The accuracy.\n",
        "  ax = plt.subplot(122)\n",
        "  ax.set_title('Eval Accuracy')\n",
        "  ax.set_ylabel('accuracy')\n",
        "  ax.set_xlabel('epoch')\n",
        "  ymin = np.min(eval_accuracy[ignore_first_n_epochs:])\n",
        "  ymax = np.max(eval_accuracy[ignore_first_n_epochs:])\n",
        "  ax.set_ylim(ymin-(ymax-ymin)/10, ymax+(ymax-ymin)/10)\n",
        "  ax.plot(range(1, epochs+1), eval_accuracy, \"-o\", linewidth=3)\n",
        "\n",
        "class Progress:\n",
        "    \"\"\"Text mode progress bar.\n",
        "    Usage:\n",
        "            p = Progress(30)\n",
        "            p.step()\n",
        "            p.step()\n",
        "            p.step(reset=True) # to restart form 0%\n",
        "    The progress bar displays a new header at each restart.\"\"\"\n",
        "    def __init__(self, maxi, size=100, msg=\"\"):\n",
        "        \"\"\"\n",
        "        :param maxi: the number of steps required to reach 100%\n",
        "        :param size: the number of characters taken on the screen by the progress bar\n",
        "        :param msg: the message displayed in the header of the progress bar\n",
        "        \"\"\"\n",
        "        self.maxi = maxi\n",
        "        self.p = self.__start_progress(maxi)()  # `()`: to get the iterator from the generator.\n",
        "        self.header_printed = False\n",
        "        self.msg = msg\n",
        "        self.size = size\n",
        "        self.lock = Lock()\n",
        "\n",
        "    def step(self, reset=False):\n",
        "        with self.lock:\n",
        "            if reset:\n",
        "                self.__init__(self.maxi, self.size, self.msg)\n",
        "            if not self.header_printed:\n",
        "                self.__print_header()\n",
        "            next(self.p)\n",
        "\n",
        "    def __print_header(self):\n",
        "        print()\n",
        "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
        "        print(format_string.format(self.msg))\n",
        "        self.header_printed = True\n",
        "\n",
        "    def __start_progress(self, maxi):\n",
        "        def print_progress():\n",
        "            # Bresenham's algorithm. Yields the number of dots printed.\n",
        "            # This will always print 100 dots in max invocations.\n",
        "            dx = maxi\n",
        "            dy = self.size\n",
        "            d = dy - dx\n",
        "            for x in range(maxi):\n",
        "                k = 0\n",
        "                while d >= 0:\n",
        "                    print('=', end=\"\", flush=True)\n",
        "                    k += 1\n",
        "                    d -= dx\n",
        "                d += dy\n",
        "                yield k\n",
        "            # Keep yielding the last result if there are too many steps.\n",
        "            while True:\n",
        "              yield k\n",
        "\n",
        "        return print_progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xgS_8nDDIu8"
      },
      "source": [
        "## MNIST 데이터세트 다운로드 및 준비하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbN7rmuF0VFB"
      },
      "outputs": [],
      "source": [
        "(x_train, train_labels), (x_test, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, train_labels))\n",
        "train_data = train_data.map(lambda x,y: (tf.expand_dims(tf.cast(x, tf.float32)/255.0, axis=-1),\n",
        "                                         tf.one_hot(y, depth=10)))\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "train_data = train_data.batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_data = train_data.cache()\n",
        "train_data = train_data.shuffle(5000, reshuffle_each_iteration=True)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices((x_test, test_labels))\n",
        "test_data = test_data.map(lambda x,y: (tf.expand_dims(tf.cast(x, tf.float32)/255.0, axis=-1),\n",
        "                                         tf.one_hot(y, depth=10)))\n",
        "test_data = test_data.batch(10000)\n",
        "test_data = test_data.cache()\n",
        "\n",
        "(one_batch, one_batch_labels) = next(iter(train_data)) # just one batch\n",
        "(all_test_data, all_test_labels) = next(iter(test_data)) # all in one batch since batch size is 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuZTo7SM3W_n"
      },
      "source": [
        "## 훈련 구성하기\n",
        "\n",
        "이 노트북에서는 데모 목적으로 간단한 모델을 만들고 훈련합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vbKB4yZ3aTL"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters.\n",
        "JAX_EPOCHS = 3\n",
        "TF_EPOCHS = 7\n",
        "STEPS_PER_EPOCH = len(train_labels)//BATCH_SIZE\n",
        "LEARNING_RATE = 0.01\n",
        "LEARNING_RATE_EXP_DECAY = 0.6\n",
        "\n",
        "# The learning rate schedule for JAX (with Optax).\n",
        "jlr_decay = optax.exponential_decay(LEARNING_RATE, transition_steps=STEPS_PER_EPOCH, decay_rate=LEARNING_RATE_EXP_DECAY, staircase=True)\n",
        "\n",
        "# THe learning rate schedule for TensorFlow.\n",
        "tflr_decay = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=LEARNING_RATE, decay_steps=STEPS_PER_EPOCH, decay_rate=LEARNING_RATE_EXP_DECAY, staircase=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od3sMwQxtC34"
      },
      "source": [
        "## Flax를 사용하여 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ybqQF2zd2QX"
      },
      "outputs": [],
      "source": [
        "class ConvModel(flax.linen.Module):\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x, train):\n",
        "    x = flax.linen.Conv(features=12, kernel_size=(3,3), padding=\"SAME\", use_bias=False)(x)\n",
        "    x = flax.linen.BatchNorm(use_running_average=not train, use_scale=False, use_bias=True)(x)\n",
        "    x = x.reshape((x.shape[0], -1))  # flatten\n",
        "    x = flax.linen.Dense(features=200, use_bias=True)(x)\n",
        "    x = flax.linen.BatchNorm(use_running_average=not train, use_scale=False, use_bias=True)(x)\n",
        "    x = flax.linen.Dropout(rate=0.3, deterministic=not train)(x)\n",
        "    x = flax.linen.relu(x)\n",
        "    x = flax.linen.Dense(features=10)(x)\n",
        "    #x = flax.linen.log_softmax(x)\n",
        "    return x\n",
        "\n",
        "  # JAX differentiation requires a function `f(params, other_state, data, labels)` -> `loss` (as a single number).\n",
        "  # `jax.grad` will differentiate it against the fist argument.\n",
        "  # The user must split trainable and non-trainable variables into `params` and `other_state`.\n",
        "  # Must pass a different RNG key each time for the dropout mask to be different.\n",
        "  def loss(self, params, other_state, rng, data, labels, train):\n",
        "    logits, batch_stats = self.apply({'params': params, **other_state},\n",
        "                                     data,\n",
        "                                     mutable=['batch_stats'],\n",
        "                                     rngs={'dropout': rng},\n",
        "                                     train=train)\n",
        "    # The loss averaged across the batch dimension.\n",
        "    loss = optax.softmax_cross_entropy(logits, labels).mean()\n",
        "    return loss, batch_stats\n",
        "\n",
        "  def predict(self, state, data):\n",
        "    logits = self.apply(state, data, train=False) # predict and accuracy disable dropout and use accumulated batch norm stats (train=False)\n",
        "    probabilities = flax.linen.log_softmax(logits)\n",
        "    return probabilities\n",
        "\n",
        "  def accuracy(self, state, data, labels):\n",
        "    probabilities = self.predict(state, data)\n",
        "    predictions = jnp.argmax(probabilities, axis=-1)\n",
        "    dense_labels = jnp.argmax(labels, axis=-1)\n",
        "    accuracy = jnp.equal(predictions, dense_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cr0FRNFtHN4"
      },
      "source": [
        "## 훈련 단계 함수 작성하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmDwApcpgZzw"
      },
      "outputs": [],
      "source": [
        "# The training step.\n",
        "@partial(jax.jit, static_argnums=[0]) # this forces jax.jit to recompile for every new model\n",
        "def train_step(model, state, optimizer_state, rng, data, labels):\n",
        "\n",
        "  other_state, params = state.pop('params') # differentiate only against 'params' which represents trainable variables\n",
        "  (loss, batch_stats), grads = jax.value_and_grad(model.loss, has_aux=True)(params, other_state, rng, data, labels, train=True)\n",
        "\n",
        "  updates, optimizer_state = optimizer.update(grads, optimizer_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  new_state = state.copy(add_or_replace={**batch_stats, 'params': params})\n",
        "\n",
        "  rng, _ = jax.random.split(rng)\n",
        "\n",
        "  return new_state, optimizer_state, rng, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr16g6NzV4O9"
      },
      "source": [
        "## 훈련 루프 작성하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbl5w-KUV7Qw"
      },
      "outputs": [],
      "source": [
        "def train(model, state, optimizer_state, train_data, epochs, losses, avg_losses, eval_losses, eval_accuracies):\n",
        "  p = Progress(STEPS_PER_EPOCH)\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    # This is where the learning rate schedule state is stored in the optimizer state.\n",
        "    optimizer_step = optimizer_state[1].count\n",
        "\n",
        "    # Run an epoch of training.\n",
        "    for step, (data, labels) in enumerate(train_data):\n",
        "      p.step(reset=(step==0))\n",
        "      state, optimizer_state, rng, loss = train_step(model, state, optimizer_state, rng, data.numpy(), labels.numpy())\n",
        "      losses.append(loss)\n",
        "    avg_loss = np.mean(losses[-step:])\n",
        "    avg_losses.append(avg_loss)\n",
        "\n",
        "    # Run one epoch of evals (10,000 test images in a single batch).\n",
        "    other_state, params = state.pop('params')\n",
        "    # Gotcha: must discard modified batch_stats here\n",
        "    eval_loss, _ = model.loss(params, other_state, rng, all_test_data.numpy(), all_test_labels.numpy(), train=False)\n",
        "    eval_losses.append(eval_loss)\n",
        "    eval_accuracy = model.accuracy(state, all_test_data.numpy(), all_test_labels.numpy())\n",
        "    eval_accuracies.append(eval_accuracy)\n",
        "\n",
        "    print(\"\\nEpoch\", epoch, \"train loss:\", avg_loss, \"eval loss:\", eval_loss, \"eval accuracy\", eval_accuracy, \"lr:\", jlr_decay(optimizer_step))\n",
        "\n",
        "  return state, optimizer_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGB3W5g0Wt1H"
      },
      "source": [
        "## 모델 및 옵티마이저 생성하기(Optax 사용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW5mkmCWtN8W"
      },
      "outputs": [],
      "source": [
        "# The model.\n",
        "model = ConvModel()\n",
        "state = model.init({'params':jax.random.PRNGKey(0), 'dropout':jax.random.PRNGKey(0)}, one_batch, train=True) # Flax allows a separate RNG for \"dropout\"\n",
        "\n",
        "# The optimizer.\n",
        "optimizer = optax.adam(learning_rate=jlr_decay) # Gotcha: it does not seem to be possible to pass just a callable as LR, must be an Optax Schedule\n",
        "optimizer_state = optimizer.init(state['params'])\n",
        "\n",
        "losses=[]\n",
        "avg_losses=[]\n",
        "eval_losses=[]\n",
        "eval_accuracies=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJdsKghBNF"
      },
      "source": [
        "## 모델 훈련하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmcofTTBZSIb"
      },
      "outputs": [],
      "source": [
        "new_state, new_optimizer_state = train(model, state, optimizer_state, train_data, JAX_EPOCHS+TF_EPOCHS, losses, avg_losses, eval_losses, eval_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_20vgvDXB5r"
      },
      "outputs": [],
      "source": [
        "display_train_curves(losses, avg_losses, eval_losses, eval_accuracies, len(eval_losses), STEPS_PER_EPOCH, ignore_first_n=1*STEPS_PER_EPOCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lT3cdENCBzL"
      },
      "source": [
        "## 모델 부분적으로 훈련하기\n",
        "\n",
        "계속해서 TensorFlow에서 모델 훈련을 이어 가겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT-xqj5N7C6L"
      },
      "outputs": [],
      "source": [
        "model = ConvModel()\n",
        "state = model.init({'params':jax.random.PRNGKey(0), 'dropout':jax.random.PRNGKey(0)}, one_batch, train=True) # Flax allows a separate RNG for \"dropout\"\n",
        "\n",
        "# The optimizer.\n",
        "optimizer = optax.adam(learning_rate=jlr_decay) # LR must be an Optax LR Schedule\n",
        "optimizer_state = optimizer.init(state['params'])\n",
        "\n",
        "losses, avg_losses, eval_losses, eval_accuracies = [], [], [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa362HMDbzDE"
      },
      "outputs": [],
      "source": [
        "state, optimizer_state = train(model, state, optimizer_state, train_data, JAX_EPOCHS, losses, avg_losses, eval_losses, eval_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IyZtUPPCt0y"
      },
      "outputs": [],
      "source": [
        "display_train_curves(losses, avg_losses, eval_losses, eval_accuracies, len(eval_losses), STEPS_PER_EPOCH, ignore_first_n=1*STEPS_PER_EPOCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNtlSaOCCumB"
      },
      "source": [
        "## 추론에 필요한 만큼만 저장하기\n",
        "\n",
        "JAX 모델을 배포하는 것이 목표인 경우(`model.predict()`를 사용하여 추론을 실행할 수 있도록) 단순히 [SavedModel](https://www.tensorflow.org/guide/saved_model)로 이를 내보내는 것만으로도 충분합니다. 이 섹션에서는 이를 수행하는 방법을 설명합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O653B3-5H8FL"
      },
      "outputs": [],
      "source": [
        "# Test data with a different batch size to test polymorphic shapes.\n",
        "x, y = next(iter(train_data.unbatch().batch(13)))\n",
        "\n",
        "m = tf.Module()\n",
        "# Wrap the JAX state in `tf.Variable` (needed when calling the converted JAX function.\n",
        "state_vars = tf.nest.map_structure(tf.Variable, state)\n",
        "# Keep the wrapped state as flat list (needed in TensorFlow fine-tuning).\n",
        "m.vars = tf.nest.flatten(state_vars)\n",
        "# Convert the desired JAX function (`model.predict`).\n",
        "predict_fn = jax2tf.convert(model.predict, polymorphic_shapes=[\"...\", \"(b, 28, 28, 1)\"])\n",
        "# Wrap the converted function in `tf.function` with the correct `tf.TensorSpec` (necessary for dynamic shapes to work).\n",
        "@tf.function(autograph=False, input_signature=[tf.TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32)])\n",
        "def predict(data):\n",
        "    return predict_fn(state_vars, data)\n",
        "m.predict = predict\n",
        "tf.saved_model.save(m, \"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HFx67zStgvo"
      },
      "outputs": [],
      "source": [
        "# Test the converted function.\n",
        "print(\"Converted function predictions:\", np.argmax(m.predict(x).numpy(), axis=-1))\n",
        "# Reload the model.\n",
        "reloaded_model = tf.saved_model.load(\"./\")\n",
        "# Test the reloaded converted function (the result should be the same).\n",
        "print(\"Reloaded  function predictions:\", np.argmax(reloaded_model.predict(x).numpy(), axis=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEk8wv4HJu94"
      },
      "source": [
        "## 모두 저장하기\n",
        "\n",
        "전체 내보내기가 목표인 경우(미세 조정, 융합 등을 위해 모델을 TensorFlow로 가져올 계획인 경우 유용함) 이 섹션에서는 모델을 저장하여 다음과 같은 메서드에 액세스할 수 있는 방법을 설명합니다.\n",
        "\n",
        "- model.predict\n",
        "- model.accuracy\n",
        "- model.loss(train=True/False 부울, 드롭아웃을 위한 RNG 및 BatchNorm 상태 업데이트 포함)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mty52pmvDDp"
      },
      "outputs": [],
      "source": [
        "from collections import abc\n",
        "\n",
        "def _fix_frozen(d):\n",
        "  \"\"\"Changes any mappings (e.g. frozendict) back to dict.\"\"\"\n",
        "  if isinstance(d, list):\n",
        "    return [_fix_frozen(v) for v in d]\n",
        "  elif isinstance(d, tuple):\n",
        "    return tuple(_fix_frozen(v) for v in d)\n",
        "  elif not isinstance(d, abc.Mapping):\n",
        "    return d\n",
        "  d = dict(d)\n",
        "  for k, v in d.items():\n",
        "    d[k] = _fix_frozen(v)\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HEsKNXbCwXw"
      },
      "outputs": [],
      "source": [
        "class TFModel(tf.Module):\n",
        "  def __init__(self, state, model):\n",
        "    super().__init__()\n",
        "\n",
        "    # Special care needed for the train=True/False parameter in the loss\n",
        "    @jax.jit\n",
        "    def loss_with_train_bool(state, rng, data, labels, train):\n",
        "      other_state, params = state.pop('params')\n",
        "      loss, batch_stats = jax.lax.cond(train,\n",
        "                                       lambda state, data, labels: model.loss(params, other_state, rng, data, labels, train=True),\n",
        "                                       lambda state, data, labels: model.loss(params, other_state, rng, data, labels, train=False),\n",
        "                                       state, data, labels)\n",
        "      # must use JAX to split the RNG, therefore, must do it in a @jax.jit function\n",
        "      new_rng, _ = jax.random.split(rng)\n",
        "      return loss, batch_stats, new_rng\n",
        "\n",
        "    self.state_vars = tf.nest.map_structure(tf.Variable, state)\n",
        "    self.vars = tf.nest.flatten(self.state_vars)\n",
        "    self.jax_rng = tf.Variable(jax.random.PRNGKey(0))\n",
        "\n",
        "    self.loss_fn = jax2tf.convert(loss_with_train_bool, polymorphic_shapes=[\"...\", \"...\", \"(b, 28, 28, 1)\", \"(b, 10)\", \"...\"])\n",
        "    self.accuracy_fn = jax2tf.convert(model.accuracy, polymorphic_shapes=[\"...\", \"(b, 28, 28, 1)\", \"(b, 10)\"])\n",
        "    self.predict_fn = jax2tf.convert(model.predict, polymorphic_shapes=[\"...\", \"(b, 28, 28, 1)\"])\n",
        "\n",
        "  # Must specify TensorSpec manually for variable batch size to work\n",
        "  @tf.function(autograph=False, input_signature=[tf.TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32)])\n",
        "  def predict(self, data):\n",
        "    # Make sure the TfModel.predict function implicitly use self.state_vars and not the JAX state directly\n",
        "    # otherwise, all model weights would be embedded in the TF graph as constants.\n",
        "    return self.predict_fn(self.state_vars, data)\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32),\n",
        "                                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],\n",
        "               autograph=False)\n",
        "  def train_loss(self, data, labels):\n",
        "      loss, batch_stats, new_rng = self.loss_fn(self.state_vars, self.jax_rng, data, labels, True)\n",
        "      # update batch norm stats\n",
        "      flat_vars = tf.nest.flatten(self.state_vars['batch_stats'])\n",
        "      flat_values = tf.nest.flatten(batch_stats['batch_stats'])\n",
        "      for var, val in zip(flat_vars, flat_values):\n",
        "        var.assign(val)\n",
        "      # update RNG\n",
        "      self.jax_rng.assign(new_rng)\n",
        "      return loss\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32),\n",
        "                                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],\n",
        "               autograph=False)\n",
        "  def eval_loss(self, data, labels):\n",
        "      loss, batch_stats, new_rng = self.loss_fn(self.state_vars, self.jax_rng, data, labels, False)\n",
        "      return loss\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32),\n",
        "                                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],\n",
        "               autograph=False)\n",
        "  def accuracy(self, data, labels):\n",
        "    return self.accuracy_fn(self.state_vars, data, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znJrAVpcxO9u"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model.\n",
        "tf_model = TFModel(state, model)\n",
        "\n",
        "# Save the model.\n",
        "tf.saved_model.save(tf_model, \"./\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y02DHEwTjNzV"
      },
      "source": [
        "## 모델 다시 로드하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i75yS3v2jPpM"
      },
      "outputs": [],
      "source": [
        "reloaded_model = tf.saved_model.load(\"./\")\n",
        "\n",
        "# Test if it works and that the batch size is indeed variable.\n",
        "x,y = next(iter(train_data.unbatch().batch(13)))\n",
        "print(np.argmax(reloaded_model.predict(x).numpy(), axis=-1))\n",
        "x,y = next(iter(train_data.unbatch().batch(20)))\n",
        "print(np.argmax(reloaded_model.predict(x).numpy(), axis=-1))\n",
        "\n",
        "print(reloaded_model.accuracy(one_batch, one_batch_labels))\n",
        "print(reloaded_model.accuracy(all_test_data, all_test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiwEAwQmlx1x"
      },
      "source": [
        "## 변환된 JAX 모델을 TensorFlow에서 계속 훈련하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MubFcO_jl2vE"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=tflr_decay)\n",
        "\n",
        "# Set the iteration step for the learning rate to resume from where it left off in JAX.\n",
        "optimizer.iterations.assign(len(eval_losses)*STEPS_PER_EPOCH)\n",
        "\n",
        "p = Progress(STEPS_PER_EPOCH)\n",
        "\n",
        "for epoch in range(JAX_EPOCHS, JAX_EPOCHS+TF_EPOCHS):\n",
        "\n",
        "  # This is where the learning rate schedule state is stored in the optimizer state.\n",
        "  optimizer_step = optimizer.iterations\n",
        "\n",
        "  for step, (data, labels) in enumerate(train_data):\n",
        "    p.step(reset=(step==0))\n",
        "    with tf.GradientTape() as tape:\n",
        "      #loss = reloaded_model.loss(data, labels, True)\n",
        "      loss = reloaded_model.train_loss(data, labels)\n",
        "      grads = tape.gradient(loss, reloaded_model.vars)\n",
        "      optimizer.apply_gradients(zip(grads, reloaded_model.vars))\n",
        "      losses.append(loss)\n",
        "  avg_loss = np.mean(losses[-step:])\n",
        "  avg_losses.append(avg_loss)\n",
        "\n",
        "  eval_loss = reloaded_model.eval_loss(all_test_data.numpy(), all_test_labels.numpy()).numpy()\n",
        "  eval_losses.append(eval_loss)\n",
        "  eval_accuracy = reloaded_model.accuracy(all_test_data.numpy(), all_test_labels.numpy()).numpy()\n",
        "  eval_accuracies.append(eval_accuracy)\n",
        "\n",
        "  print(\"\\nEpoch\", epoch, \"train loss:\", avg_loss, \"eval loss:\", eval_loss, \"eval accuracy\", eval_accuracy, \"lr:\", tflr_decay(optimizer.iterations).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50V1FSmI6UTk"
      },
      "outputs": [],
      "source": [
        "display_train_curves(losses, avg_losses, eval_losses, eval_accuracies, len(eval_losses), STEPS_PER_EPOCH, ignore_first_n=2*STEPS_PER_EPOCH)\n",
        "\n",
        "# The loss takes a hit when the training restarts, but does not go back to random levels.\n",
        "# This is likely caused by the optimizer momentum being reinitialized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7lSziW0K0ny"
      },
      "source": [
        "## 다음 단계\n",
        "\n",
        "[JAX](https://jax.readthedocs.io/en/latest/index.html) 및 [Flax](https://flax.readthedocs.io/en/latest)에 대한 자세한 내용은 상세 가이드와 예제가 포함된 해당 문서 웹사이트에서 확인할 수 있습니다. JAX를 처음 접하는 경우 [JAX 101 튜토리얼](https://jax.readthedocs.io/en/latest/jax-101/index.html)과 [Flax 퀵스타트](https://flax.readthedocs.io/en/latest/getting_started.html)를 확인하세요. JAX 모델을 TensorFlow 형식으로 변환하는 방법에 대한 자세한 내용은 GitHub에서 [jax2tf](https://github.com/google/jax/tree/main/jax/experimental/jax2tf) 유틸리티를 확인하세요. 브라우저에서 실행할 수 있도록 JAX 모델을 변환하는 데 관심이 있는 경우 [JAX on the Web with TensorFlow.js](https://blog.tensorflow.org/2022/08/jax-on-web-with-tensorflowjs.html)를 방문하세요. TensorFlow Lite에서 실행하는 JAX 모델을 준비하려면 [TFLite용 JAX 모델 변환](https://www.tensorflow.org/lite/examples/jax_conversion/overview) 가이드를 참조하세요."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "jax2tf.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
